#Object Detection in 20 Years: A Survey

##abstract
objection detection的20世纪90年代到2022年的领域回顾，包括了历史上的里程碑检测器、检测数据集、度量、检测系统的基本构建块、加速技术和最新的最先进的检测方法。

##keyword
计算机视觉，卷积神经网络，深度学习，目标检测，技术进化

##introduction
objection detection做的是计算机视觉应用程序最基本的知识之一：对象在哪里？

两个重要的指标就是速度和精度

物体检测的挑战有许多：物体旋转、小物体、定位、密集和有遮挡、加速检测

在第二节中，我们回顾了20年来目标检测的发展。在第三节中，我们回顾了目标检测中的加速技术。第四节回顾了近三年的最新检测方法。第五节对本文进行总结，并对进一步的研究方向进行了深入分析。

##objection detection in 20 years
###目标检测路线图（road map of object detection）

总的目标检测的路线图如下图所示

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1726463381089.png)

2000年之前，传统检测大多是基于手工特征检测进行的

2001年，__VJ检测__(名字来源于人名)之后通过滑动窗口，结合__积分图像__，__特征选择__，__级联检测__来检测人脸

2005年，HOG检测（histogram of oriented gradients直方图定向梯度）改进了 __尺度不变特征变换__ 和 __形状上下文__ 

2008年，__DPM检测__(可变形零件模型)可以理解为对HOG检测器的拓展，加上了一种“分治”的思想，检测车就从检测车窗、车身、车轮开始，这部分工作，又称“star-model”。

2010年，因为手工特征的性能饱和目标检测进入平台期。

2012年，__CNN(卷积神经网络)__对学习图像的__鲁棒__和高级特征表示。2014年被引入 __带有CNN特征的区域(region with CNN features, rcnn)__之后目标检测迅速发展

__RCNN__：用选择性搜索提取 __对象候选框(object candidate boxes)__  将每个box缩放为固定的大小，输入在__ImageNet__ 中预训练好的 __CNN模型__提取特征，最后使用__线性支持向量机__来预测每个区域内物体的存在大幅提升了性能。但是缺点也很明显--有很多重叠的box

2014年，__SPPNet(空间金字塔池网络)__ 他的贡献在于SPP层能使得CNN生成固定长度表示而不用缩放大小，避免了重复计算卷积特征，快20倍

2015年，__Fast RCNN__ 在相同的net配置之下训练了检测器和边界box回归器。结合了SPPnet和RCNN的优点

2015年,__Faster RCNN__ 引入了一个__区域提议网络（RPN）__ 使得区域提议几乎零成本

2017年，__特征金字塔网络--FPNS__ FPN之前，大多数基于深度学习的检测器仅在网络顶层的特征映射上运行检测。虽然CNN的深层特征有利于类别识别，但不利于对象的定位。为此，FPN开发了一种具有__横向连接的自顶向下架构__，用于在所有尺度上构建高级语义。

__基于CNN的单阶段检测器__：大多数__两阶段检测器__遵循从粗到精的处理范式。粗的是努力提高召回能力，而细的是在粗的检测基础上细化定位，更强调区分能力。它们可以很容易地获得高精度而不需要任何花哨的东西，但由于它们的速度差和巨大的复杂性，在工程中很少使用。相反，单阶段检测器可以在一步推理中检索到所有对象。它们很受移动设备的喜爱，具有实时和易于部署的特性，但在检测密集和小物体时，它们的性能明显受到影响

2015年，__YOLO__ 遵循单个神经网络应用于整个图像，并且将图像划分为多个区域，预测每个区域的边界框和概率。

2015年，__SSD(Single-Shot Multibox Detector)__，引入了多参考点和多分辨率检测技术，提高了小物体的检测精度

2017年，__RetinaNet__ 引入了一个新的损失函数--__焦点损失__，通过重塑标准交叉熵损失，使检测器在训练过程中更多地关注难分类的错误示例

__CornerNet__ : 将任务视为关键点(盒子的角落)预测问题。获取关键点后，利用额外的嵌入信息对角点进行解耦重组，形成边界框。

2019年，__CenterNet__ 遵循基于关键点的检测范式，将对象视为单个点(对象的中心)，并基于参考中心点回归其所有属性(如大小、方向、位置和姿态)

2020年，__DERT__ 将目标检测视为一个集合预测问题，并提出了一个端到端的变压器检测网络

###目标检测的数据集和指标
####数据集
在过去十年中，已经发布了许多知名的检测数据集，包括__PASCAL VOC挑战(例如，VOC2007, VOC2012)， ImageNet大规模视觉识别挑战(例如，ILSVRC2014) [56]， MS-COCO检测挑战[57]，开放图像数据集[58]，[59]，Objects365[60]__等数据集。

####指标
早期使用__“每窗口误报率”(FPPW)__ 来衡量，之后又推出了__"每图像误报"(FPPI)__ 的评估指标
近年来，最常用的检测评估是__“平均精度(AP)”__
AP被定义为不同召回下的平均检测精度，通常以特定类别的方式进行评估。所有类别的 __mAP平均值__ 通常用作性能的最终度量。为了测量目标定位精度，使用预测框与地面真实值之间的交集(IoU)来验证它是否大于预定义的阈值，例如0.5。如果是，该对象将被标识为“检测到”，否则为“未发现”。随后，__0.5 iou mAP__成为了物体检测的实际度量标准

###目标检测的技术演变
####多尺度检测技术的发展:
“不同尺寸”和“不同长宽比”__目标的多尺度检测__是目标检测的主要技术挑战之一。近20年来，多尺度探测经历了多个历史时期，如图所示。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1726504797221.png)

一开始再__VJ检测器__之后研究者比较关注一个直观的检测方式 __特征金字塔+滑动窗口__ 他们经常在图像上滑动一个固定大小的检测窗口，很少注意“不同的长宽比”。为了检测具有更复杂外观的物体

__混合模型__是当时的一种解决方案，即针对不同长宽比的物体训练多个检测器。除此之外，基于样本的检测通过为每个对象实例(样本)训练单独的模型提供了另一种解决方案。

__使用对象建议进行检测(Detection With Object proposals)__:对象建议指的是一组与类无关的参考框，这些参考框可能包含任何对象。使用目标建议进行检测有助于避免在图像上进行穷举滑动窗口搜索。

__深度回归和无锚点检测(Deep Regression and Anchor-Free Detection)__:近年来，随着GPU计算能力的提高，多尺度检测变得越来越直接和蛮力。使用深度回归解决多尺度问题的思路变得简单，即基于深度学习特征直接预测边界框的坐标

2018年之后，研究人员开始从__关键点检测__的角度思考目标检测问题。这些方法通常遵循两种思路:一种是__基于分组__的方法，检测关键点(角、中心或代表性点)，然后进行对象分组;另一种是__将一个对象视为一个或多个点__，然后在点的参考下回归对象属性(大小，比例等)

__多参考/多分辨率检测(Multireference/Multiresolution Detection)__ 
主要思想是首先定义一组参考(多参考),在图像的每个位置锚定(包括框和点)，然后根据这些参考预测检测框。另一种流行的技术是__多分辨率检测__即通过在网络的不同层检测不同尺度的对象。

多参考点和多分辨率检测已经成为当今最先进的目标检测系统的两个基本组成部分。

####语境启动的技术演变:
视觉对象通常与周围环境嵌入在一个典型的语境中。我们的大脑利用物体和环境之间的联系来促进视觉感知和认知。长期以来，语境启动一直被用来提高检测能力。图显示了上下文启动在目标检测中的演化过程

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1726506180061.png)

__Local Context检测__:Local Context是指被检测对象周围区域的视觉信息。提供一些背景信息可以有效提高检测准确性

__Detection With Global Context(全局上下文检测)__:全局上下文利用场景配置作为对象检测的额外信息源。一种方法是利用__深度卷积、扩展卷积、可变形卷积和池化操作__来接收更大的接受场。第二种方法是__将全局上下文视为一种顺序信息__，并使用循环神经网络进行学习

__上下文交互__:上下文交互是指在视觉元素之间传递的约束和依赖关系。最近的一些改进可以分为两类，其中__第一类是探索单个对象之间的关系__，__第二个是探索对象和场景之间的依赖关系__。

####Hard Negative Mining (难例挖掘)
检测器的训练本质上是一个不平衡学习问题。在基于滑动窗口的检测器的情况下，背景和目标之间的不平衡可能达到极端的107:1 在这种情况下，使用所有的背景对训练是有害的，因为大量简单的否定会淹没学习过程。__HNM__旨在克服这一问题。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1726576694803.png)

__Bootstrap__:Bootstrap在目标检测中是指从一小部分背景样本开始训练，然后迭代地增加新的错误分类样本的一组训练技术。

__HNM in Deep Learning-Based Detectors__:为了缓解训练过程中的数据不平衡问题，Faster RCNN和YOLO等检测器只是在正窗和负窗之间平衡权重。然而，研究人员后来发现，这并不能完全解决不平衡问题[25]。为此，__自举(Bootstrap)__在2016年之后被重新引入到目标检测中

####4损失函数的技术演变(Evolution of Loss Function)
损失函数衡量模型与数据的匹配程度(即预测与真实标签的偏差)。计算损失产生模型权重的梯度，随后可以通过反向传播更新以更好地适应数据。

__分类损失(Classification Loss)__:用于__评估预测类别与实际类别的偏离程度__.YOLOv1[20]和YOLOv2采用均__方误差(mse)/L2损失__。之后，通常使用__交叉熵(cross-entropy, CE)损失__。L2损耗是欧氏空间中的一种度量，而CE损耗可以度量分布差异(称为似然的一种形式)。分类预测是一个概率，因此CE损失优于L2损失，误分类代价大，梯度消失效应小。为了提高分类效率，提出了__标签平滑来增强模型泛化能力__，解决噪声标签上的过置信度问题，设计焦点损失来解决类别不平衡和分类难度差异问题

__定位损失(Localization Loss)__:L1损耗,公式如下：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1726578001553.png)

其中x表示目标值与预测值之间的差值。在计算误差时，上述损失将表示一个边界框的四个数字(x, y, w, h)作为自变量;相等的平滑L1值将具有完全不同的IoU值;因此，IoU损失

	IoU loss = − log(IoU).

