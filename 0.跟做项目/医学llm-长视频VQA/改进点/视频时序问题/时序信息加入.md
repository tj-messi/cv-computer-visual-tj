#时序信息idea想法

手术的阶段时序信息应该和Answer关系比较大，但是24年SoTa的采样方法却是随机采样，相当于模型只能对每个视频当中的随机帧建立起图像-label的关系。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20250214194544.png)

我们考虑同期视频+图片的多模态融合方法

首先定义视频处理模块

	class VideoFeatureExtractor(nn.Module):
	    def __init__(self):
	        super(VideoFeatureExtractor, self).__init__()
	        self.visual_encoder = ViTModel.from_pretrained("/home/test/PitVQA-main/local_VIT")
	
	    def forward(self, video_frames):
	        # Process video frames (assuming video_frames is a tensor of shape [batch_size, num_frames, channels, height, width])
	        video_features = self.visual_encoder(video_frames).last_hidden_state
	        return video_features.mean(dim=1)  # Average over the frames to get a single feature vector per video

然后在PitVQA中加入model

	# Video feature extraction module
    self.video_feature_extractor = VideoFeatureExtractor()

Forward里面载入处理embedding

	# Video feature extraction
    video_embeds = self.video_feature_extractor(video)


##dataloader里面

取出video，每一帧都做tensor化转变

		# 最后根据95分位进行裁剪和填充
        raw_video = padding_and_slicing(raw_video,frames_num)
        # print("img_location: ",img_loc," video-length: ",len(raw_video))
        transformed_video = [self.transform(frame) for frame in raw_video]

根据每个txt附近相似图片整合为视频，视频帧长度统计做统计分析95位作为分界线执行padding或者slicing

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/text_len_summary.png)

这个slice截止值为37

所以video的shape:([64,37,3,224,224])

##train里面

取出video

	for i, (_, images, videos ,questions, labels) in enumerate(train_dataloader, 0):

堆叠成tensor然后to device

	outputs = model(image=images.to(device),video = torch.stack(videos).to(device), question=questions)  # questions is a tuple