{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.8.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"d:/Data/Graduate Content/Ophthalmology Papers/Artificial_intelligence/Knowledge_Graph/Retinal_Disease_Knowledge_Graph_Building/04 Neo4j_Building/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import chardet\n",
    "import torch\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.models import RotatE, ComplEx, ConvE\n",
    "from pykeen.losses import NSSALoss\n",
    "from pykeen.regularizers import PowerSumRegularizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "\n",
    "def detect_file_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return chardet.detect(f.read(10000))['encoding']\n",
    "\n",
    "def extract_properties(properties_str):\n",
    "    props = {}\n",
    "    matches = re.finditer(r'(\\w+)\\s*:\\s*([^,}]+)', properties_str)\n",
    "    for match in matches:\n",
    "        key = match.group(1).strip().lower()\n",
    "        value = match.group(2).strip()\n",
    "        props[key] = value\n",
    "    return props\n",
    "\n",
    "def parse_cypher_relationship(line):\n",
    "    pattern = (\n",
    "        r'\\(:\\s*(?P<head_type>[\\w_]+)\\s*\\{(?P<head_props>.*?)\\}\\)'\n",
    "        r'\\s*-\\s*\\[:(?P<relation>[^\\]]+)\\]\\s*->\\s*'\n",
    "        r'\\(:\\s*(?P<tail_type>[\\w_]+)\\s*\\{(?P<tail_props>.*?)\\}\\)'\n",
    "    )\n",
    "    match = re.search(pattern, line)\n",
    "    if not match:\n",
    "        print(f\"格式错误: 不符合基本结构\\n问题行: {line}\")\n",
    "        return None\n",
    "    try:\n",
    "        head_type = match.group('head_type').strip()\n",
    "        head_props = extract_properties(match.group('head_props'))\n",
    "        relation = match.group('relation').strip()\n",
    "        tail_type = match.group('tail_type').strip()\n",
    "        tail_props = extract_properties(match.group('tail_props'))\n",
    "        required = ['name', 'id', 'label']\n",
    "        for prop in required:\n",
    "            if prop not in head_props:\n",
    "                raise ValueError(f\"头实体缺少必要属性: {prop}\")\n",
    "            if prop not in tail_props:\n",
    "                raise ValueError(f\"尾实体缺少必要属性: {prop}\")\n",
    "        return {\n",
    "            \"head_id\": head_props['id'],\n",
    "            \"head_name\": head_props['name'],\n",
    "            \"head_type\": head_type,\n",
    "            \"head_label\": head_props['label'],\n",
    "            \"tail_id\": tail_props['id'],\n",
    "            \"tail_name\": tail_props['name'],\n",
    "            \"tail_type\": tail_type,\n",
    "            \"tail_label\": tail_props['label'],\n",
    "            \"relation\": relation\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"解析失败: {str(e)}\\n问题行: {line}\")\n",
    "        return None\n",
    "\n",
    "def standardize_data(df):\n",
    "    df['head_id'] = df['head_id'].astype(str).str.strip()\n",
    "    df['tail_id'] = df['tail_id'].astype(str).str.strip()\n",
    "    df['head_name'] = df['head_name'].str.strip()\n",
    "    df['tail_name'] = df['tail_name'].str.strip()\n",
    "    invalid_ids = df[(df['head_id'] == '') | (df['tail_id'] == '')]\n",
    "    if not invalid_ids.empty:\n",
    "        print(f\"警告: 发现{len(invalid_ids)}个空ID，已自动过滤\")\n",
    "        df = df[(df['head_id'] != '') & (df['tail_id'] != '')]\n",
    "    return df\n",
    "\n",
    "def train_and_evaluate(train_tf, test_tf, model_name=\"RotatE\"):\n",
    "    \"\"\"训练和评估不同模型\"\"\"\n",
    "    # 模型配置 (移除了device参数)\n",
    "    model_config = {\n",
    "        \"embedding_dim\": 256,\n",
    "        \"loss\": NSSALoss(margin=12.0),\n",
    "        \"regularizer\": PowerSumRegularizer(p=2.0, weight=1e-5),\n",
    "    }\n",
    "    \n",
    "    if model_name == \"RotatE\":\n",
    "        model_config.update({\n",
    "            \"embedding_dim\": 200,\n",
    "            \"loss\": NSSALoss(margin=9.0)\n",
    "        })\n",
    "    elif model_name == \"ComplEx\":\n",
    "        model_config.update({\n",
    "            \"embedding_dim\": 256,\n",
    "            \"loss\": NSSALoss(margin=5.0)\n",
    "        })\n",
    "    elif model_name == \"ConvE\":\n",
    "        model_config.update({\n",
    "            \"embedding_dim\": 200,\n",
    "            \"input_channels\": 1,\n",
    "            \"output_channels\": 32,\n",
    "            \"embedding_height\": 10,\n",
    "            \"embedding_width\": 20,\n",
    "            \"kernel_height\": 3,\n",
    "            \"kernel_width\": 3,\n",
    "            \"input_dropout\": 0.2,\n",
    "            \"feature_map_dropout\": 0.2,\n",
    "            \"output_dropout\": 0.3\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n开始训练{model_name}模型...\")\n",
    "    result = pipeline(\n",
    "        training=train_tf,\n",
    "        testing=test_tf,\n",
    "        model=model_name,\n",
    "        model_kwargs=model_config,\n",
    "        training_kwargs=dict(\n",
    "            num_epochs=200,\n",
    "            batch_size=256,\n",
    "            checkpoint_name=f\"{model_name}_checkpoint.pt\",\n",
    "            checkpoint_frequency=20\n",
    "        ),\n",
    "        evaluation_kwargs=dict(batch_size=128),\n",
    "        random_seed=42,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  # device参数移到这里\n",
    "    )\n",
    "    \n",
    "    # 详细评估\n",
    "    evaluator = RankBasedEvaluator()\n",
    "    metric_results = evaluator.evaluate(\n",
    "        model=result.model,\n",
    "        mapped_triples=test_tf.mapped_triples,\n",
    "        batch_size=128,\n",
    "        additional_filter_triples=[train_tf.mapped_triples]\n",
    "    )\n",
    "    \n",
    "    return result, metric_results\n",
    "\n",
    "def main():\n",
    "    file_path = \"E:\\\\研究生内容\\\\眼科-论文\\\\人工智能\\\\知识图谱\\\\眼底疾病数据库\\\\Relation.csv\"\n",
    "\n",
    "    try:\n",
    "        encoding = detect_file_encoding(file_path)\n",
    "        print(f\"文件编码: {encoding}\")\n",
    "        with open(file_path, \"r\", encoding=encoding, errors='replace') as f:\n",
    "            lines = [line.strip() for line in f if line.strip()]\n",
    "        print(f\"\\n读取到 {len(lines)} 行数据\")\n",
    "        for i, line in enumerate(lines[:3], 1):\n",
    "            print(f\"[样本{i}] {line}\")\n",
    "    except Exception as e:\n",
    "        print(f\"文件读取失败: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    relations = []\n",
    "    error_log = []\n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        parsed = parse_cypher_relationship(line)\n",
    "        if parsed:\n",
    "            relations.append(parsed)\n",
    "        else:\n",
    "            error_log.append({\"line_num\": line_num, \"content\": line})\n",
    "\n",
    "    print(f\"\\n解析结果:\")\n",
    "    print(f\"√ 成功解析: {len(relations)} 行\")\n",
    "    print(f\"× 解析失败: {len(error_log)} 行\")\n",
    "\n",
    "    if not relations:\n",
    "        print(\"错误: 没有解析出任何有效关系\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(relations)\n",
    "    df = standardize_data(df)\n",
    "    \n",
    "    # 使用实体名称作为标识符\n",
    "    triples = [(row['head_name'], row['relation'], row['tail_name']) for _, row in df.iterrows()]\n",
    "    \n",
    "    # 分割训练测试集\n",
    "    train_triples, test_triples = train_test_split(triples, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 创建TriplesFactory\n",
    "    train_tf = TriplesFactory.from_labeled_triples(np.array(train_triples, dtype=str))\n",
    "    test_tf = TriplesFactory.from_labeled_triples(np.array(test_triples, dtype=str), \n",
    "                                                entity_to_id=train_tf.entity_to_id,\n",
    "                                                relation_to_id=train_tf.relation_to_id)\n",
    "\n",
    "    # 训练和评估多个模型\n",
    "    model_results = {}\n",
    "    for model_name in [\"RotatE\", \"ComplEx\", \"ConvE\"]:\n",
    "        result, metrics = train_and_evaluate(train_tf, test_tf, model_name)\n",
    "        model_results[model_name] = {\n",
    "            \"result\": result,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "        print(f\"\\n{model_name} 模型评估结果:\")\n",
    "        print(f\"IHMR: {metrics.get_metric(name='both.realistic.inverse_harmonic_mean_rank')}\")\n",
    "        print(f\"Hits@10: {metrics.get_metric(name='both.realistic.hits_at_10')}\")\n",
    "\n",
    "    # 选择最佳模型\n",
    "    best_model = max(model_results.items(), \n",
    "                    key=lambda x: x[1]['metrics'].get_metric('both.realistic.inverse_harmonic_mean_rank'))\n",
    "    print(f\"\\n最佳模型: {best_model[0]}\")\n",
    "    \n",
    "    # 保存最佳模型的嵌入向量\n",
    "    save_embeddings(best_model[1]['result'], train_tf, \"best_model\")\n",
    "\n",
    "def save_embeddings(result, triples_factory, prefix):\n",
    "    \"\"\"保存嵌入向量\"\"\"\n",
    "    # 实体嵌入\n",
    "    entity_embeddings = result.model.entity_representations[0]\n",
    "    entity_ids = list(triples_factory.entity_to_id.keys())\n",
    "    entity_vectors = entity_embeddings(torch.arange(len(entity_ids))).detach().numpy()\n",
    "    entity_df = pd.DataFrame(entity_vectors, index=entity_ids)\n",
    "    entity_df.index.name = \"entity\"\n",
    "    entity_df.to_csv(f\"D:\\Data\\Graduate Content\\Ophthalmology Papers\\Artificial intelligence\\Knowledge Graph\\Retinal Disease\\{prefix}_entity_embeddings.csv\")\n",
    "    \n",
    "    # 关系嵌入\n",
    "    relation_embeddings = result.model.relation_representations[0]\n",
    "    relation_ids = list(triples_factory.relation_to_id.keys())\n",
    "    relation_vectors = relation_embeddings(torch.arange(len(relation_ids))).detach().numpy()\n",
    "    relation_df = pd.DataFrame(relation_vectors, index=relation_ids)\n",
    "    relation_df.index.name = \"relation\"\n",
    "    relation_df.to_csv(f\"D:\\Data\\Graduate Content\\Ophthalmology Papers\\Artificial intelligence\\Knowledge Graph\\Retinal Disease\\{prefix}_relation_embeddings.csv\")\n",
    "    \n",
    "    print(f\"\\n已保存{prefix}的嵌入向量到指定目录\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
