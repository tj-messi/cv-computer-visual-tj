##git clone

	git clone https://github.com/SuperMedIntel/Medical-SAM2.git

云服务器git clone可能出现链接超时

可以这样

	git clone https:xxx

加上一个gitclone.com的镜像

#搭建环境

	conda env create -f environment.yml

#下载权重

	bash download_ckpts.sh

#下载2D数据集

Download pre-processed REFUGE datase

	wget https://huggingface.co/datasets/jiayuanz3/REFUGE/resolve/main/REFUGE.zip

	unzip REFUGE.zip

这个也经常链接不上，可以
	
1、下载到本地,再上传 

	网站：https://huggingface.co/datasets/jiayuanz3/REFUGE/resolve/main/REFUGE.zip


#跑训练

	 python train_2d.py -net sam2 -exp_name REFUGE_MedSAM2 -vis 1 -sam_ckpt ./checkpoints/sam2_hiera_small.pt -sam_config sam2_hiera_s -image_size 1024 -out_size 1024 -b 4 -val_freq 1 -dataset REFUGE -data_path ./data/REFUGE

此时会：

	UserWarning: Flash Attention is disabled as it requires a GPU with Ampere (8.0) CUDA capability.


检查CUDA的能力版本

	 python -c "from torch import cuda; print(cuda.get_device_capability())"

本服务器的是-7.0

此外还有一个报错是

	INFO:root:Namespace(net='sam2', encoder='vit_b', exp_name='REFUGE_MedSAM2', vis=True, train_vis=False, prompt='bbox', prompt_freq=2, pretrain=None, val_freq=1, gpu=True, gpu_device=0, image_size=1024, out_size=1024, distributed='none', dataset='REFUGE', sam_ckpt='./checkpoints/sam2_hiera_small.pt', sam_config='sam2_hiera_s', video_length=2, b=4, lr=0.0001, weights=0, multimask_output=1, memory_bank_size=16, data_path='./data/REFUGE', path_helper={'prefix': 'logs/REFUGE_MedSAM2_2024_11_30_02_23_31', 'ckpt_path': 'logs/REFUGE_MedSAM2_2024_11_30_02_23_31/Model', 'log_path': 'logs/REFUGE_MedSAM2_2024_11_30_02_23_31/Log', 'sample_path': 'logs/REFUGE_MedSAM2_2024_11_30_02_23_31/Samples'})

	Namespace(net='sam2', encoder='vit_b', exp_name='REFUGE_MedSAM2', vis=True, train_vis=False, prompt='bbox', prompt_freq=2, pretrain=None, val_freq=1, gpu=True, gpu_device=0, image_size=1024, out_size=1024, distributed='none', dataset='REFUGE', sam_ckpt='./checkpoints/sam2_hiera_small.pt', sam_config='sam2_hiera_s', video_length=2, b=4, lr=0.0001, weights=0, multimask_output=1, memory_bank_size=16, data_path='./data/REFUGE', path_helper={'prefix': 'logs/REFUGE_MedSAM2_2024_11_30_02_23_31', 'ckpt_path': 'logs/REFUGE_MedSAM2_2024_11_30_02_23_31/Model', 'log_path': 'logs/REFUGE_MedSAM2_2024_11_30_02_23_31/Log', 'sample_path': 'logs/REFUGE_MedSAM2_2024_11_30_02_23_31/Samples'})
	Traceback (most recent call last):                                                 
	  File "/root/zjz-医疗影像大模型/Medical-SAM2/train_2d.py", line 127, in <module>
	    main()
	  File "/root/zjz-医疗影像大模型/Medical-SAM2/train_2d.py", line 100, in main
	    tol, (eiou, edice) = function.validation_sam(args, nice_test_loader, epoch, net, writer)
	                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/root/zjz-医疗影像大模型/Medical-SAM2/func_2d/function.py", line 335, in validation_sam
	    vision_feats_temp = vision_feats[-1].permute(1, 0, 2).view(B, -1, 64, 64) 
	                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

这个把view改成reshape就行

跑实验可能出现如下报错：该报错是一个典型的 CUDA 内存不足错误 (CUDA out of memory)。它说明当前 GPU 上的可用显存不足以分配所需的内存块（512 MiB），导致程序无法继续运行。

	torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 291.38 MiB is free. Process 34168 has 2.54 GiB memory in use. Process 34416 has 2.75 GiB memory in use. Process 8873 has 4.60 GiB memory in use. Process 9328 has 437.39 MiB memory in use. Process 9334 has 437.39 MiB memory in use. Process 9335 has 501.39 MiB memory in use. Process 9333 has 437.39 MiB memory in use. Process 9325 has 501.39 MiB memory in use. Process 9326 has 501.39 MiB memory in use. Process 9329 has 437.39 MiB memory in use. Process 9327 has 501.39 MiB memory in use. Process 9324 has 501.39 MiB memory in use. Process 9330 has 437.39 MiB memory in use. Process 71518 has 2.67 GiB memory in use. Process 43033 has 4.22 GiB memory in use. Process 53284 has 2.98 GiB memory in use. Process 38693 has 6.79 GiB memory in use. Of the allocated memory 6.20 GiB is allocated by PyTorch, and 227.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

此时可以检查gpu占用：

	nvidia-smi

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1732986471734.png)

可以参考如下教程

[](https://blog.csdn.net/qq_45193872/article/details/122643769?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%87%E6%8D%A2%E4%BD%BF%E7%94%A8%E7%9A%84gpu&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-122643769.142^v100^pc_search_result_base5&spm=1018.2226.3001.4187)

	GPUdevice = torch.device('cuda', 3)  # 改为使用卡3

或者

	 parser.add_argument('-gpu_device', type=int, default=0, help='use which gpu')

修改成

	 parser.add_argument('-gpu_device', type=int, default=3, help='use which gpu')

然后代码里面很多部分也需要修改这样就能修改使用的GPU