#Qilin-Med-VL部署

##先下载huggingface模型

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1732108932122.png)

可以直接执行第二条命令

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1732109160997.png)

！服务器上下载huggingface容易波动

可以这样解决

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241120220016.png)

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241120220029.png)

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241120220043.png)

然后就可以

	huggingface-cli download --resume-download “模型名字”

执行下载

##下载github源码

直接

	 git clone https://github.com/williamliujl/Qilin-Med-VL.git

即可

##下载微调可视化llama-factory

云服务器git clone可能出现链接超时

可以这样

	git clone https://gitclone.com/github.com/hiyouga/LLaMA-Factory.git

加上一个gitclone.com的镜像

##给llama-factory开虚拟环境

	conda create -n llama_factory python=3.9 -y

	pip install -e ".[torch,metrics]"

安装python包

-e (可编辑模式)：

表示以“可编辑模式”安装该包。
包安装后与本地代码目录保持链接。如果你修改了源码，改动会立即生效，而无需重新安装。
".[torch,metrics]"：

. 表示当前目录（假设当前目录是一个包含 Python 包的项目）。
"[torch,metrics]" 是可选依赖组的名字。它来自 setup.py 或 pyproject.toml 文件中定义的 extras_require 字段。
在这种情况下，torch 和 metrics 是两个可选的依赖组，它们通常是一些额外的库或工具。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1732167003421.png)

