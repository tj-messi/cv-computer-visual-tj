# 基因匹配

## 基因数据匹配

将以下两个目录的spot-ST基因匹配

    /media/cbtil/T7 Shield/NMI/data/Visiumhdmousebrain4_8/spot_ST

    /media/cbtil/T7 Shield/NMI/data/Visium_mouse_brain/spot_ST

这两个目录基因分别叫做visiumhd和visium

其中visium的基因有30000+，visiumhd的基因有8000+

进行同名匹配：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/38b79ca734884968c576408e75d394a.png)

其中每20个进行匹配，如果有对不上的就直接全部舍弃

未匹配上的：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1747483382940.png)

二次检查：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1747483427254.png)


批量保存：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1747483852217.png)

## test

    /media/cbtil/T7 Shield/NMI/code111111/sample_final_forvisium.py

数据传过来

    scp -r cbtil@10.241.64.165:"/media/cbtil/T7\ Shield/NMI/data/Visium_mouse_brain_matched"  /home/zeiler/NMI/data


修改：

    #data_root: '/home/cbtil/ST_proj/data/Breast_cancer/'
    data_root: '/home/zeiler/NMI/data/Visium_mouse_brain_matched/'

    args.all_gene    = 8000#（记得换）
    args.gene_num    = 20
    args.batch_size  = 1
    args.SR_times    = 10

    model_dirs = glob.glob(
        os.path.join("logsVisiumhdmousebrain",#(记得换)
                     f"{args.dataset_use}/{args.SR_times}X/G*")
    )

    # load patch info
    ds_info = Xenium5k2(
        data_root=args.data_root,
        dataset_use=args.dataset_use,
        SR_times=args.SR_times,
        status='Test',
        gene_num=args.gene_num,
        all_gene=args.all_gene,
        gene_order=gene_order,
        gene_name_order=gene_name_order
    )#（记得换）

下采样：

    pr = sample.permute(0,2,3,1).cpu().numpy()[0]
    pr = normalize_prediction(pr)      
    print(pr.shape)
    # 将 (256, 256, 20) 转换为 (1, 20, 256, 256)，N=1, C=20, H=256, W=256
    pr_tensor = torch.tensor(pr).unsqueeze(0).permute(0, 3, 1, 2)  # 变为 (1, 20, 256, 256)

    # 下采样：将尺寸缩小到目标大小 (26, 26)
    pr_resized = F.interpolate(pr_tensor, size=(26, 26), mode='bilinear', align_corners=False)

    # 将下采样后的张量转回 numpy 数组
    pr = pr_resized.squeeze(0).permute(1, 2, 0).cpu().numpy()  # 变回 (26, 26, 20)

    # save
    save_sample_images(hr, pr, pid, out_samps)

开始跑test

    /home/zeiler/NMI/code/sample_final_forvisium.py

4080-4100开始报错

为了检查后续模型是否都会load报错

写一个try except 跑下去

    for model_dir in model_dirs:
        try:
            bn = os.path.basename(model_dir)
            m = re.search(r'G(\d+)-(\d+)', bn)
            if not m:
                continue
            start, end = map(int, m.groups())
            gene_order = np.load(gene_order_path)[start:end]
            gene_name_order = np.loadtxt(gene_name_order_path, dtype=str)[start:end]

            # checkpoint
            cks = glob.glob(os.path.join(model_dir, "model*.pt"))
            if not cks:
                continue
            model_numbers = [int(re.search(r'model(\d+)\.pt', ck).group(1)) for ck in cks]

            # 排序并选择第二大的模型
            model_numbers.sort(reverse=True)  # 按降序排序
            second_largest_model = model_numbers[1]  # 获取第二大的模型

            model_path = os.path.join(model_dir, f"model{second_largest_model:06d}.pt")

            script_name = f"Ours-{args.dataset_use}/{args.SR_times}X/G{start}-{end}"
            results_dir = os.path.join(base_out, "TEST_Result_full_visium_mousebrain", script_name)
            os.makedirs(results_dir, exist_ok=True)
            logger.configure(dir=results_dir)
            logger.log(f"=== Testing genes {start}-{end} ===")

            # load model
            model, diffusion = sr_create_model_and_diffusion(args)
            model.load_state_dict(dist_util.load_state_dict(model_path, map_location="cpu", weights_only=False))
            model.to(dist_util.dev())
            if args.use_fp16:
                model.convert_to_fp16()
            model.eval()

            # load patch info
            ds_info = Xenium5k7(
                data_root=args.data_root,
                dataset_use=args.dataset_use1,
                SR_times=args.SR_times,
                status='Test',
                gene_num=args.gene_num,
                all_gene=args.all_gene,
                gene_order=gene_order,
                gene_name_order=gene_name_order
            )  # （记得换）
            patch_info = ds_info.selected_patches
            patch_info.sort(key=lambda x: (int(x[1].split('_')[0]), int(x[1].split('_')[1])))

            data_gen = load_superres_data(
                args.batch_size,
                args.data_root,
                args.dataset_use1,
                'Test',
                args.SR_times,
                args.gene_num,
                args.all_gene,
                gene_order,
                gene_name_order
            )

            # metrics CSV
            csv_path = os.path.join(results_dir, "metrics.csv")
            with open(csv_path, "w") as f:
                f.write("SampleID,RMSE,SSIM,BinaryCos\n")

            # output samples
            out_samps = os.path.join(results_dir, "samples")
            os.makedirs(out_samps, exist_ok=True)

            # iterate patches
            for sid, pid in patch_info:
                try:
                    mk = next(data_gen)
                    hr = mk['low_res']

                    hr = hr.permute(0, 2, 3, 1).cpu().numpy()[0]
                    mk = {k: v.to(dist_util.dev()) for k, v in mk.items()}

                    with torch.no_grad():
                        sample = diffusion.ddim_sample_loop(
                            model,
                            (1, args.gene_num, mk['WSI_5120'].shape[2], mk['WSI_5120'].shape[3]),
                            clip_denoised=args.clip_denoised,
                            model_kwargs=mk
                        )
                    pr = sample.permute(0, 2, 3, 1).cpu().numpy()[0]
                    pr = normalize_prediction(pr)
                    print(pr.shape)

                    pr_tensor = torch.tensor(pr).unsqueeze(0).permute(0, 3, 1, 2)

                    pr_resized = F.interpolate(pr_tensor, size=(26, 26), mode='bilinear', align_corners=False)

                    pr = pr_resized.squeeze(0).permute(1, 2, 0).cpu().numpy()

                    # save
                    save_sample_images(hr, pr, pid, out_samps)

                    # compute & print
                    r, s, cb = compute_metrics(hr, pr, threshold=0.1)
                    print(f"[G{start}-{end}] patch {pid}: RMSE={r:.4f}, SSIM={s:.4f}, BinaryCos={cb:.4f}")

                    with open(csv_path, "a") as f:
                        f.write(f"{pid},{r:.4f},{s:.4f},{cb:.4f}\n")

                except Exception as e:
                    # Log the error and continue with the next patch
                    logger.log(f"Error processing patch {pid}: {e}")
                    continue

            # 拼大图
            create_spatial_composite(results_dir, patch_info)

        except Exception as e:
            # Log the error for the current model_dir and continue with the next one
            logger.log(f"Error processing model directory {model_dir}: {e}")
            continue

    logger.log("All done!")

最后输出报错log：

4080-4900 报错

5080-5900 报错

7960-8000 报错

其他正常

## 补充有问题model并补上测试

zelier服务器 0号卡泡4000-5000

zelier服务器 1号卡泡5000-6000




