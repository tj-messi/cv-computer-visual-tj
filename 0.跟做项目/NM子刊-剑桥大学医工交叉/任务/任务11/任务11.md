# 280基因test

## 任务描述

280基因model在cb3服务器

把cb服务器上的sample_final.py文件传入cb3服务器

    import os
    import re
    import glob
    import argparse
    import yaml
    import numpy as np
    import torch
    import torch.nn.functional as F
    import torch.distributed as dist
    from torch.utils.data import DataLoader
    from guided_diffusion.img import load_data, Xenium_humanbreast
    from guided_diffusion import dist_util, logger
    from guided_diffusion.script_util import sr_create_model_and_diffusion, add_dict_to_argparser
    from skimage.metrics import structural_similarity
    from sklearn.metrics.pairwise import cosine_similarity
    import matplotlib.pyplot as plt
    from mpi4py import MPI

    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    torch.cuda.set_device(rank)  # 如果只有一张卡, rank 一般都是 0

    def create_argparser():
        parser = argparse.ArgumentParser()
        parser.add_argument("--config", help="Path to YAML configuration file")
        with open('/home/cbtil-3/QNF/code/config/config_test.yaml') as f:
            cfg = yaml.safe_load(f)
        add_dict_to_argparser(parser, cfg)
        return parser

    def compute_metrics(gt: np.ndarray, pred: np.ndarray, threshold=0.01):
        """只算 RMSE、SSIM、二值化余弦相似度（通道平均）"""
        gt_thr = gt.copy(); pred_thr = pred.copy()
        gt_thr[gt_thr < threshold] = 0
        pred_thr[pred_thr < threshold] = 0

        gt_bin = (gt_thr > 0).astype(np.float32)
        pred_bin = (pred_thr > 0).astype(np.float32)

        rmse_list, ssim_list, cos_bin_list = [], [], []
        C = gt.shape[-1]
        for i in range(C):
            g = gt_thr[..., i]; p = pred_thr[..., i]
            rmse_list.append(np.sqrt(np.mean((g - p)**2)))
            ssim_list.append(structural_similarity(g, p, data_range=1.0))
            gb = gt_bin[..., i].ravel().reshape(1, -1)
            pb = pred_bin[..., i].ravel().reshape(1, -1)
            if gb.any() and pb.any():
                cos_bin_list.append(cosine_similarity(gb, pb)[0][0])
        return (
            float(np.mean(rmse_list)) if rmse_list else 0.0,
            float(np.mean(ssim_list)) if ssim_list else 0.0,
            float(np.mean(cos_bin_list)) if cos_bin_list else 0.0
        )

    def normalize_prediction(pred: np.ndarray,
                            initial_cutoff=90, step=60, decay=10, min_cutoff=80):
        """通道逐步下剪裁 + min-max 归一化"""
        for k in range(pred.shape[-1]):
            ch = pred[..., k]
            if not ch.any():
                pred[..., k] = 0
                continue
            cutoff = max(initial_cutoff - decay * (k // step), min_cutoff)
            lb = np.percentile(ch, cutoff)
            ch[ch < lb] = lb
            mn, mx = ch.min(), ch.max()
            pred[..., k] = (ch - mn) / (mx - mn + 1e-8) if mx > mn else 0
        return pred

    def save_sample_images(gt: np.ndarray, pred: np.ndarray, sid: str, output_dir: str):
        """保存彩色图和 .npy"""
        color_dir = os.path.join(output_dir, "color_images")
        npy_dir   = os.path.join(output_dir, "Channel_npy_files")
        os.makedirs(color_dir, exist_ok=True)
        os.makedirs(npy_dir, exist_ok=True)

        # 彩色
        for i in range(gt.shape[-1]):
            plt.imsave(os.path.join(color_dir, f"{sid}_gt_{i}.png"),
                    gt[..., i], cmap="viridis")
            plt.imsave(os.path.join(color_dir, f"{sid}_pred_{i}.png"),
                    pred[..., i], cmap="viridis")
        # 整包 .npy
        np.save(os.path.join(npy_dir, f"{sid}_gt_all_genes.npy"), gt)
        np.save(os.path.join(npy_dir, f"{sid}_pred_all_genes.npy"), pred)

    def create_spatial_composite(results_dir: str, patch_info, gene_num=20, image_size=256):
        """拼接所有 patch 的 .npy，生成大图并保存（按 列→y, 行→x 排布）"""
        comp_dir = os.path.join(results_dir, "spatial_composite")
        os.makedirs(comp_dir, exist_ok=True)

        overlap = 0.5
        stride = int(image_size * (1 - overlap))

        # 把 patch_id "row_col" 解析成 (col, row)
        coords = []
        for _, pid in patch_info:
            row, col = map(int, pid.split("_"))
            coords.append((col, row, pid))

        if not coords:
            return

        ys, xs = zip(*[(y, x) for y, x, _ in coords])
        min_y, max_y = min(ys), max(ys)
        min_x, max_x = min(xs), max(xs)

        H = (max_y - min_y) * stride + image_size
        W = (max_x - min_x) * stride + image_size

        gt_comp = np.zeros((H, W, gene_num), dtype=np.float32)
        pr_comp = np.zeros((H, W, gene_num), dtype=np.float32)

        npy_dir = os.path.join(results_dir, "samples", "Channel_npy_files")
        for y, x, pid in coords:
            y0 = (y - min_y) * stride
            x0 = (x - min_x) * stride

            gt_path = os.path.join(npy_dir, f"{pid}_gt_all_genes.npy")
            pr_path = os.path.join(npy_dir, f"{pid}_pred_all_genes.npy")
            if not os.path.exists(gt_path) or not os.path.exists(pr_path):
                continue

            g = np.load(gt_path)
            p = np.load(pr_path)
            h, w, _ = g.shape

            gt_comp[y0:y0+h, x0:x0+w] = g
            pr_comp[y0:y0+h, x0:x0+w] = p

        # 保存拼接结果
        np.save(os.path.join(comp_dir, "gt_all_genes.npy"), gt_comp)
        np.save(os.path.join(comp_dir, "pred_all_genes.npy"), pr_comp)

        # 可视化第0通道
        for arr, name in [(gt_comp, "gt"), (pr_comp, "pred")]:
            ch0 = arr[..., 0]
            ch0n = ch0 / (ch0.max() + 1e-8)
            plt.imsave(os.path.join(comp_dir, f"{name}_ch0.png"), ch0n, cmap="viridis")


    def load_superres_data(batch_size, data_root, dataset_use, status,
                        SR_times, gene_num, all_gene, gene_order, gene_name_order):
        """统一的 DataLoader 生成器，每次 yield (hr_tensor, model_kwargs_dict)"""
        ds = load_data(
            data_root=data_root,
            dataset_use=dataset_use,
            status=status,
            SR_times=SR_times,
            gene_num=gene_num,
            all_gene=all_gene,
            gene_order=gene_order,
            gene_name_order=gene_name_order
        )
        loader = DataLoader(
            ds, batch_size=batch_size, shuffle=False,
            num_workers=4, drop_last=False, pin_memory=True
        )
        for batch in loader:
            # 标准 Xeniumhd_mouse_kidney case
            SR_ST, spot_ST, WSI_5120, WSI_320, gene_class, Gene_index_map, gene_name_features, metadata_feature = batch
            hr_tensor = SR_ST
            mk = {
                "low_res": spot_ST,
                "WSI_5120": WSI_5120,
                "WSI_320": WSI_320,
                "gene_class": gene_class,
                "Gene_index_map": Gene_index_map,
                "gene_name_features": gene_name_features,
                "metadata_feature": metadata_feature
            }
            yield hr_tensor, mk

    def main():
        args = create_argparser().parse_args()
        dist_util.setup_dist()

        # 固定测试参数
        args.all_gene    = 280
        args.gene_num    = 20
        args.batch_size  = 1
        args.SR_times    = 10
        args.dataset_use = 'Xenium_BreastCancer'
        base_out = os.path.abspath(os.path.dirname(__file__))

        gene_order_path      = os.path.join(args.data_root, args.dataset_use, 'gene_order.npy')
        gene_name_order_path = os.path.join(args.data_root, args.dataset_use, 'gene_names.txt')

        model_dirs = glob.glob(
            os.path.join("logs_Xenium_humanbreast",
                        f"{args.dataset_use}/{args.SR_times}X/G*")
        )
        model_dirs.sort(key=lambda p: int(re.search(r'G(\d+)-', os.path.basename(p)).group(1)))

        for model_dir in model_dirs:
            bn = os.path.basename(model_dir)
            m = re.search(r'G(\d+)-(\d+)', bn)
            if not m:
                continue
            start, end = map(int, m.groups())
            gene_order      = np.load(gene_order_path)[start:end]
            gene_name_order = np.loadtxt(gene_name_order_path, dtype=str)[start:end]

            # checkpoint
            cks = glob.glob(os.path.join(model_dir, "model*.pt"))
            if not cks:
                continue
            step = max(int(re.search(r'model(\d+)\.pt', ck).group(1)) for ck in cks)
            model_path = os.path.join(model_dir, f"model{step:06d}.pt")

            script_name = f"Ours-{args.dataset_use}/{args.SR_times}X/G{start}-{end}"
            results_dir = os.path.join(base_out, "TEST_Result_full", script_name)
            os.makedirs(results_dir, exist_ok=True)
            logger.configure(dir=results_dir)
            logger.log(f"=== Testing genes {start}-{end} ===")

            # load model
            model, diffusion = sr_create_model_and_diffusion(args)
            model.load_state_dict(dist_util.load_state_dict(model_path, map_location="cpu"))
            model.to(dist_util.dev())
            if args.use_fp16:
                model.convert_to_fp16()
            model.eval()

            # load patch info
            ds_info = Xenium_humanbreast(
                data_root=args.data_root,
                dataset_use=args.dataset_use,
                SR_times=args.SR_times,
                status='Test',
                gene_num=args.gene_num,
                all_gene=args.all_gene,
                gene_order=gene_order,
                gene_name_order=gene_name_order
            )
            patch_info = ds_info.selected_patches
            patch_info.sort(key=lambda x: (int(x[1].split('_')[0]), int(x[1].split('_')[1])))

            data_gen = load_superres_data(
                args.batch_size,
                args.data_root,
                args.dataset_use,
                'Test',
                args.SR_times,
                args.gene_num,
                args.all_gene,
                gene_order,
                gene_name_order
            )

            # metrics CSV
            csv_path = os.path.join(results_dir, "metrics.csv")
            with open(csv_path, "w") as f:
                f.write("SampleID,RMSE,SSIM,BinaryCos\n")

            # output samples
            out_samps = os.path.join(results_dir, "samples")
            os.makedirs(out_samps, exist_ok=True)

            # iterate patches
            for sid, pid in patch_info:
                hr_tensor, mk = next(data_gen)

                # ensure 256×256
                if args.SR_times == 5 or hr_tensor.shape[-2:] != (256,256):
                    hr_tensor = F.interpolate(hr_tensor, size=(256,256), mode='bilinear', align_corners=False)
                hr = hr_tensor.permute(0,2,3,1).cpu().numpy()[0]

                mk = {k: v.to(dist_util.dev()) for k,v in mk.items()}

                with torch.no_grad():
                    sample = diffusion.ddim_sample_loop(
                        model,
                        (1, args.gene_num,
                        mk['WSI_5120'].shape[2],
                        mk['WSI_5120'].shape[3]),
                        clip_denoised=args.clip_denoised,
                        model_kwargs=mk
                    )
                pr = sample.permute(0,2,3,1).cpu().numpy()[0]
                pr = normalize_prediction(pr)

                # save
                save_sample_images(hr, pr, pid, out_samps)

                # compute & print
                r, s, cb = compute_metrics(hr, pr, threshold=0.1)
                print(f"[G{start}-{end}] patch {pid}: RMSE={r:.4f}, SSIM={s:.4f}, BinaryCos={cb:.4f}")

                with open(csv_path, "a") as f:
                    f.write(f"{pid},{r:.4f},{s:.4f},{cb:.4f}\n")

            # 拼大图
            create_spatial_composite(results_dir, patch_info)

        logger.log("All done!")

    if __name__ == "__main__":
        main()

## 调这里看效果

    def normalize_prediction(pred: np.ndarray,
                            initial_cutoff=90, step=60, decay=10, min_cutoff=80):
        """通道逐步下剪裁 + min-max 归一化"""
        for k in range(pred.shape[-1]):
            ch = pred[..., k]
            if not ch.any():
                pred[..., k] = 0
                continue
            cutoff = max(initial_cutoff - decay * (k // step), min_cutoff)
            lb = np.percentile(ch, cutoff)
            ch[ch < lb] = lb
            mn, mx = ch.min(), ch.max()
            pred[..., k] = (ch - mn) / (mx - mn + 1e-8) if mx > mn else 0
        return pred

## 跑起来看指标

跑起来:SSIM指标不能低于0.4

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/27826d3541cc197fbf80c3194b2c24d.png)

文件夹名字：80-20-60

    initial_cutoff=80, step=60, decay=20, min_cutoff=60