# 减小过拟合

## 数据增强问题：https://github.com/adamtupper/usaugment/wiki


针对超声影像的数据增强


###基础用法

在dataloader类中定义好：

	self.albu_transform = A.Compose([
	            DepthAttenuation(p=0.25),
	            GaussianShadow(p=0.25),
	            HazeArtifact(p=0.25),
	            SpeckleReduction(p=0.25),
	        ], additional_targets={"scan_mask": "mask"}) if mode == 'train' else None

之后在getitem的train模式中

	import cv2
	        if self.mode != 'train' or self.albu_transform is None:
	            # 非训练模式或无增强时，直接返回原始 buffer
	            return buffer
	
	        augmented_frames = []
	        for frame in buffer:
	            frame = frame.astype(np.uint8)
	            # 使用默认掩码（全一，与图像大小相同）
	            default_mask = np.ones_like(frame[..., 0], dtype=np.uint8)  # H W
	            transformed = self.albu_transform(image=frame, scan_mask=default_mask)
	            # 调整大小到 224x224
	            resized_frame = cv2.resize(transformed['image'], (self.crop_size, self.crop_size), 
	                                    interpolation=cv2.INTER_LINEAR)
	            augmented_frames.append(resized_frame)
	        
	        buffer = np.stack(augmented_frames)  # T H W C, H=W=224
	        buffer = [transforms.ToTensor()(frame) for frame in buffer]  # 每帧转为 C H W, float32
	        buffer = torch.stack(buffer)  # T C H W, 例如 T×3×224×224
	
	        # 显式指定 float32 并确保设备一致
	        mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32, device=buffer.device).view(1, 3, 1, 1)
	        std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32, device=buffer.device).view(1, 3, 1, 1)
	        buffer = tensor_normalize(buffer, mean, std)  # 输出 float32
	
	        # 转换为 bfloat16 以匹配混合精度训练
	        buffer = buffer.to(dtype=torch.bfloat16)
	
	        # 调整为 C T H W 以匹配验证集和模型输入
	        buffer = buffer.permute(1, 0, 2, 3)  # T C H W -> C T H W
	
	        return buffer

###注意

haze 和 depth 用法都是上面为中心，这个是不对的。

	 r = np.sqrt((xv - 0.5) ** 2 + (yv - 1.0) ** 2)

	 r = np.sqrt((xv - 0.5) ** 2 + (yv - 1.0) ** 2)

在这里修改中心点

中心点的坐标是标准化的
