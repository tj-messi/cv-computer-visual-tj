# 减小过拟合

## 数据增强问题：https://github.com/adamtupper/usaugment/wiki


针对超声影像的数据增强


###基础用法

在dataloader类中定义好：

	self.albu_transform = A.Compose([
	            DepthAttenuation(p=0.25),
	            GaussianShadow(p=0.25),
	            HazeArtifact(p=0.25),
	            SpeckleReduction(p=0.25),
	        ], additional_targets={"scan_mask": "mask"}) if mode == 'train' else None

之后在getitem的train模式中

	import cv2
	        if self.mode != 'train' or self.albu_transform is None:
	            # 非训练模式或无增强时，直接返回原始 buffer
	            return buffer
	
	        augmented_frames = []
	        for frame in buffer:
	            frame = frame.astype(np.uint8)
	            # 使用默认掩码（全一，与图像大小相同）
	            default_mask = np.ones_like(frame[..., 0], dtype=np.uint8)  # H W
	            transformed = self.albu_transform(image=frame, scan_mask=default_mask)
	            # 调整大小到 224x224
	            resized_frame = cv2.resize(transformed['image'], (self.crop_size, self.crop_size), 
	                                    interpolation=cv2.INTER_LINEAR)
	            augmented_frames.append(resized_frame)
	        
	        buffer = np.stack(augmented_frames)  # T H W C, H=W=224
	        buffer = [transforms.ToTensor()(frame) for frame in buffer]  # 每帧转为 C H W, float32
	        buffer = torch.stack(buffer)  # T C H W, 例如 T×3×224×224
	
	        # 显式指定 float32 并确保设备一致
	        mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32, device=buffer.device).view(1, 3, 1, 1)
	        std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32, device=buffer.device).view(1, 3, 1, 1)
	        buffer = tensor_normalize(buffer, mean, std)  # 输出 float32
	
	        # 转换为 bfloat16 以匹配混合精度训练
	        buffer = buffer.to(dtype=torch.bfloat16)
	
	        # 调整为 C T H W 以匹配验证集和模型输入
	        buffer = buffer.permute(1, 0, 2, 3)  # T C H W -> C T H W
	
	        return buffer

###注意

haze 和 depth 用法都是上面为中心，这个是不对的。

	 r = np.sqrt((xv - 0.5) ** 2 + (yv - 1.0) ** 2)

	 r = np.sqrt((xv - 0.5) ** 2 + (yv - 1.0) ** 2)

在这里修改中心点

中心点的坐标是标准化的

	def _aug_frame(self, buffer, args, mode='train'):
	        import cv2
	        # 将tensor转换为PIL图像
	        buffer = [transforms.ToPILImage()(frame) for frame in buffer]
	
	        # 转换为numpy array并应用超声特定增强
	        augmented_frames = []
	        if mode == 'train' and hasattr(self, 'albu_transform') and self.albu_transform is not None:
	            for frame in buffer:
	                # 转换为numpy uint8
	                frame = np.array(frame).astype(np.uint8)
	                # 创建默认掩码
	                default_mask = np.ones_like(frame[..., 0], dtype=np.uint8)  # H W
	                # 应用albumentations增强
	                transformed = self.albu_transform(image=frame, scan_mask=default_mask)
	                # 调整大小
	                resized_frame = cv2.resize(transformed['image'], (self.crop_size, self.crop_size),
	                                        interpolation=cv2.INTER_LINEAR)
	                augmented_frames.append(resized_frame)
	            buffer = augmented_frames
	        else:
	            # 如果不是训练模式，保持原始buffer
	            buffer = [np.array(frame) for frame in buffer]
	
	        resize_buffer = []
	        for frame in buffer:
	            frame = np.array(frame).astype(np.uint8)
	            aug_transform = A.Compose([
	                A.RandomResizedCrop(size=(self.crop_size, self.crop_size), scale=(0.8, 1.0), p=1),
	                A.Resize(self.crop_size, self.crop_size)
	            ])
	            resized_frame = aug_transform(image=frame)['image']
	            resize_buffer.append(resized_frame)
	        buffer = resized_frame
	
	        # 转换回tensor
	        buffer = [transforms.ToTensor()(img) for img in buffer]
	        buffer = torch.stack(buffer)  # T C H W
	        buffer = buffer.permute(0, 2, 3, 1)  # T H W C
	
	        # 标准化
	        buffer = tensor_normalize(buffer, [0.485, 0.456, 0.406],
	                                [0.229, 0.224, 0.225])
	        # T H W C -> C T H W
	        buffer = buffer.permute(3, 0, 1, 2)
	        print(buffer.size())
	        buffer = buffer.float()
	        return buffer


#dropout层

可以尝试给所有模型加上dropout层


#手工裁剪

直接针对两个图像差别裁剪

直接对内部数据中心裁剪

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/PBR60600001.png)

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/img_0006.png)


##No Train All gained

	https://github.com/WalterSimoncini/fungivision

