#6.1Mini-batch梯度下降法

###idea

原本的__全batch__整体下降法，每次需要所有的训练集进行一次下降才能更新。

__Mini-batch__就是分割出一小部分的训练样本，然后先进行一些下降，这样能加速一部分的训练

比如把500 0000的训练样本切割成500 个 1000的数据集。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1727406766823.png)

###process

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1727407049299.png)

就是执行一个小数量级的东西