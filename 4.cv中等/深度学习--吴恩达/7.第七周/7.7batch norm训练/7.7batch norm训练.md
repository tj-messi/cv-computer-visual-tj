#7.7batch norm的训练

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1727425079232.png)

BatchNorm的基本思想：能不能让每个隐层节点的输入分布固定呢？

BatchNorm为什么NB呢，关键还是效果好。不仅仅极大提升了训练速度，收敛过程大大加快，还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果。另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。

	#自己的测试
	bn=nn.BatchNorm2d(3)
	x=torch.randn([1,3,255,255])
	print(x)
	y=bn(x)
	print(y)

	print(bn.running_mean)  # 均值
	print(bn.running_var)   # 方差

最后输出结果