#3.9神经网络中的梯度下降法

基础的单隐藏层的神经网络有w[1],b[1],w[2],b[2]的参数

输入的值是nx=n[0]

还有一个损失函数

初始值记得随机初始化而不是全部设置为0

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725273025831.png)


## 反向传递的导数求解

对logistic回归的改进，出现了第二层隐藏层 Z[2],w[2],b[2]

总的导数值还是链式法则

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725273262711.png)




