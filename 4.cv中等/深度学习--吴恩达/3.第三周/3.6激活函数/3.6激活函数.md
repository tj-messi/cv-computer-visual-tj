#3.6激活函数

在之前的logistic回归识别猫图中使用的sigmoid函数就是一个激活函数，在深度学习中还有很多激活函数比如tanh函数,他在很多情况下都比sigmoid函数更加优秀，然而在二元分类中，你希望输出层y-hat输出的是0~1的时候还是选择使用sigmoid函数

##tanh函数

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725260063088.png)

tanh函数其实就是反三角函数


##修正线性单元（ReLU）

tanh和sigmoid函数都有的一个缺点是当z过大或者过小的时候，图像的函数会变得十分平缓--（斜率太低），就会减缓梯度下降法的进行，然整个过程太过平缓。

因此引入了ReLU修正线性单元

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725262539705.png)

如果不确定隐藏层要用哪一个激活函数，那么可以考虑使用ReLU作为激活函数

ReLU的缺点就是，当z小于零的时候，函数的导数为0
为了修正这个缺点，我们引进了Leaky ReLU--（带泄露的修正线性单元）

##带泄露的修正线性单元（Leaky ReLU）

数学表达式：y = max(0, x) + leak*min(0,x)  （leak是一个很小的常数，这样保留了一些负轴的值，使得负轴的信息不会全部丢失）

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725262943272.png)