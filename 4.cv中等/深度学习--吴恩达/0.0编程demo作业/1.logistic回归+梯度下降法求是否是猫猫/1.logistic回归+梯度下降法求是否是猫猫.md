#1.logistic回归+梯度下降法求是否是猫猫

##题目
![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725183005578.png)

##1.用numpy的基本函数
###1.1sigmoid基础函数

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725183878221.png)

注意sigmoid函数要使用numpy形式的矩阵

###1.2sigmoid_derivative（sigmoid梯度函数导数）

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725186929506.png)

###1.3 reshaping arrays
两个numpy的基础函数：np.shape 和 np.reshape

####np.shape
用于得到矩阵或者向量X的维度

####np.reshape
用于重新规定矩阵或者向量X的维度

####本题需要的reshape形式

然后本题目需要把（长，宽，高）形式转换成（长*宽*高，1）的形式

那么就是

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725187754135.png)

把图片转化为向量

###1.4normalize_rows 单位化（归一化）数据
就是把矩阵（向量）的数据归一化（单位化），也就是直接有一个np.linalg.norm(名字,axis=1,keepdims=True)

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725188006822.png)

###1.5broadcasting and the softmax function

####broadcasting
广播机制，也就是之前课程中提到的扩展矩阵（向量）的机制

####softmax 
理论如下

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725188191812.png)

这使得输出向量可以被解释为概率分布。Softmax函数通过指数化输入向量的每个元素，然后归一化这些指数化值来工作，从而确保输出向量的所有元素之和为1。Softmax函数在神经网络的输出层中特别有用，特别是在处理分类问题时。

##2.向量化简化for-loops

使用我numpy简化向量的时候np.dot()，



###2.1 定义L损失函数

在NumPy中实现L1损失（也称为最小绝对偏差或LAD）的向量化版本是非常直接的。L1损失是预测值与实际值之间差的绝对值的平均（或总和，取决于你的具体定义）。在向量化的版本中，我们会对所有样本的预测值与实际值之间的差的绝对值求和或平均。






