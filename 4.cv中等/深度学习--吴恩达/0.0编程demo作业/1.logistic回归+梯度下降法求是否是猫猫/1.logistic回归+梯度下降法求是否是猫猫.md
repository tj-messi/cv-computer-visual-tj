#1.logistic回归+梯度下降法求是否是猫猫

##题目
![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725183005578.png)

##1.用numpy的基本函数
###1.1sigmoid基础函数

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725183878221.png)

注意sigmoid函数要使用numpy形式的矩阵

###1.2sigmoid_derivative（sigmoid梯度函数导数）

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725186929506.png)

###1.3 reshaping arrays
两个numpy的基础函数：np.shape 和 np.reshape

####np.shape
用于得到矩阵或者向量X的维度

####np.reshape
用于重新规定矩阵或者向量X的维度

####本题需要的reshape形式

然后本题目需要把（长，宽，高）形式转换成（长*宽*高，1）的形式

那么就是

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725187754135.png)

把图片转化为向量

###1.4normalize_rows 单位化（归一化）数据
就是把矩阵（向量）的数据归一化（单位化），也就是直接有一个np.linalg.norm(名字,axis=1,keepdims=True)

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725188006822.png)

###1.5broadcasting and the softmax function

####broadcasting
广播机制，也就是之前课程中提到的扩展矩阵（向量）的机制

####softmax 
理论如下

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725188191812.png)

这使得输出向量可以被解释为概率分布。Softmax函数通过指数化输入向量的每个元素，然后归一化这些指数化值来工作，从而确保输出向量的所有元素之和为1。Softmax函数在神经网络的输出层中特别有用，特别是在处理分类问题时。

##2.向量化简化for-loops

使用我numpy简化向量的时候np.dot()，



###2.1 定义L损失函数

在NumPy中实现L1损失（也称为最小绝对偏差或LAD）的向量化版本是非常直接的。L1损失是预测值与实际值之间差的绝对值的平均（或总和，取决于你的具体定义）。在向量化的版本中，我们会对所有样本的预测值与实际值之间的差的绝对值求和或平均。


L1损失
![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725191027495.png)

L2损失
![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725191494159.png)

##3.需要的package

import numpy as np

import matplotlib.pyplot as plt

import h5py

import scipy

from PIL import Image

from scipy import ndimage

from lr_utils import load_dataset

##4.总览问题集（数据集）

###4.1先预加载数据集

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725192328559.png)

我们在图像数据集（训练集和测试集）的末尾添加了“_orig”，因为我们打算对它们进行预处理。预处理后，我们将得到train_set_x和test_set_x（而标签train_set_y和test_set_y则不需要进行任何预处理）。

train_set_x_orig和test_set_x_orig中的每一行都是一个数组，代表一张图像。你可以通过运行以下代码来可视化其中一个示例。同时，你也可以自由更改index的值并重新运行代码以查看其他图像。


###4.2 收集训练数量，测试数量，图片高度和宽度
要找到以下值：

m_train（训练样本的数量）
m_test（测试样本的数量）
num_px（训练图像的高度和宽度，因为图像是正方形的）
请记住，train_set_x_orig 是一个 numpy 数组，其形状为 (m_train, num_px, num_px, 3)。这里，m_train 是训练集中的图像数量，num_px 是图像的宽度和高度（因为图像是正方形的），而 3 表示颜色通道的数量（例如，RGB）。

要访问这些值，你可以按照以下方式操作：

m_train 可以通过 train_set_x_orig.shape[0] 获取，因为它代表了数组的第一个维度的大小，即训练图像的数量。

num_px 可以通过 train_set_x_orig.shape[1] 或 train_set_x_orig.shape[2] 获取，因为 num_px 既是图像的高度也是宽度，所以这两个索引的值是相等的。

对于 m_test，你需要假设有另一个类似的 numpy 数组 test_set_x_orig，其形状为 (m_test, num_px, num_px, 3)。因此，m_test 可以通过 test_set_x_orig.shape[0] 获取。

###4.3 展平numpy向量
为了方便处理，你应该将形状为(num_px, num_px, 3)的图像重新调整成一个numpy数组，其形状变为(num_px * num_px * 3, 1)。这样操作后，我们的训练（和测试）数据集将是一个numpy数组，其中每一列代表一个被展平（flattened）的图像。对于训练数据集，应该有m_train列；对于测试数据集，则应该有m_test列。这种处理方式使得图像数据更容易被机器学习模型处理，因为它将三维的图像数据转换成了二维的矩阵形式，其中矩阵的每一列都是一个单独的图像样本。

###4.4归一化数据
在表示彩色图像时，需要为每个像素指定红色、绿色和蓝色通道（RGB），因此像素值实际上是一个由三个介于0到255之间的数字组成的向量。

在机器学习的一个常见预处理步骤中，我们需要对数据集进行中心化和标准化处理，这意味着要从每个示例中减去整个numpy数组的平均值，然后将每个示例除以整个numpy数组的标准差。然而，对于图片数据集来说，一个更简单、更方便且效果几乎一样好的方法是直接将数据集的每一行（对应于每个像素的RGB值）除以255（即像素通道的最大值）。

这种简单的除以255的预处理方式，实际上是将像素值从原始的[0, 255]范围缩放到[0, 1]范围，这样做有利于后续机器学习模型的训练和收敛，因为大多数机器学习算法在处理小范围的输入值时表现更好。此外，这种方法避免了计算整个数据集的均值和标准差，从而节省了计算资源和时间。


###4.5注意！
你需要记住的要点：

对于新数据集进行预处理的常见步骤包括：

1.确定问题的维度和形状（例如，训练集样本数量m_train，测试集样本数量m_test，图像像素数量num_px等）。

2.重塑数据集，使得每个样本现在都是一个大小为(num_px * num_px * 3, 1)的向量。这里假设图像是彩色的，因此需要将像素数量乘以3（代表RGB三个颜色通道），并且通常这个向量会被视为二维的，其中一个维度为1，以符合某些机器学习库的要求。

3.对数据进行“标准化”处理，这通常意味着将数据缩放到一个特定的范围（如0到1之间）或使其具有特定的均值和标准差（例如，均值为0，标准差为1），以便更好地适应机器学习模型的训练过程。

##5.生成深度学习算法的基础结构
现在你会要生成一个简单的logistic回归算法，使用一个简单的NN神经结构。如下图所示。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1725196549491.png)

