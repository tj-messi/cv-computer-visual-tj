#9.8多任务学习

###多任务学习概念

在迁移学习中，你的步骤是串行的，你从任务 A 里学习只是然后迁移到任务 B 。在多任务学习中，你是同时开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务

###例子

我们来看一个例子，假设你在研发无人驾驶车辆，那么你的无人驾驶车可能需要同时检测不同的物体，比如检测行人、车辆、停车标志，还有交通灯各种其他东西。比如在左边这个例子中，图像里有个停车标志，然后图像中有辆车，但没有行人，也没有交通灯

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241002140035.png)

如果这是输入图像 x ( i ) x^{(i)}x 
(i)
  ，那么这里不再是一个标签 y ( i ) y^{(i)}y 
(i)
  ，而是有4个标签。在这个例子中，没有行人，有一辆车，有一个停车标志，没有交通灯。然后如果你尝试检测其他物体，也许 y ( i ) y^{(i)}y 
(i)
  的维数会更高，现在我们就先用4个吧，所以 y ( i ) y^{(i)}y 
(i)
  是个4×1向量。如果你从整体来看这个训练集标签和以前类似，我们将训练集的标签水平堆叠起来，像这样 y ( 1 ) y^{(1)}y 
(1)
  一直到 y ( m ) y^{(m)}y 
(m)
  


那么你现在可以做的是训练一个神经网络，来预测这些 y yy 值，你就得到这样的神经网络，输入 x xx ，现在输出是一个四维向量 y yy 。请注意，这里输出我画了四个节点，所以第一个节点就是我们想预测图中有没有行人，然后第二个输出节点预测的是有没有车，这里预测有没有停车标志，这里预测有没有交通灯，所以这里 y ^ \hat{y} 
y
^


要训练这个神经网络，你现在需要定义神经网络的损失函数，对于一个输出 y ^ \hat{y} 
y
^
​
  ，是个4维向量，对于整个训练集的平均损失：

1 m ∑ i = 1 m ∑ j = 1 4 L ( y ^ j ( i ) , y j ( i ) ) \frac1m\sum_{i=1}^m\sum_{j=1}^4L(\hat{y}^{(i)}_j,y^{(i)}_j)
m
1
​
  
i=1
∑
m
​
  
j=1
∑
4
​
 L( 
y
^
​
  
j
(i)
​
 ,y 
j
(i)
​
 )

∑ j = 1 4 L ( y ^ j ( i ) , y j ( i ) ) \sum_{j=1}^4L(\hat{y}^{(i)}_j,y^{(i)}_j)∑ 
j=1
4
​
 L( 
y
^
​
  
j
(i)
​
 ,y 
j
(i)
​
 ) 这些单个预测的损失，所以这就是对四个分量的求和，行人、车、停车标志、交通灯，而这个标志 L LL 指的是logistic损失，我们就这么写：

L ( y ^ j ( i ) , y j ( i ) ) = − y j ( i ) log ⁡ y ^ j ( i ) − ( 1 − y j ( i ) ) log ⁡ ( 1 − y ^ j ( i ) ) L(\hat{y}^{(i)}_j,y^{(i)}_j)=-y^{(i)}_j\log\hat{y}^{(i)}_j-(1-y^{(i)}_j)\log(1-\hat{y}^{(i)}_j)
L( 
y
^
​
  
j
(i)
​
 ,y 
j
(i)
​
 )=−y 
j
(i)
​
 log 
y
^
​
  
j
(i)
​
 −(1−y 
j
(i)
​
 )log(1− 
y
^
​
  
j
(i)
​
 )

整个训练集的平均损失和之前分类猫的例子主要区别在于，现在你要对 j = 1 j=1j=1 到 4 44 求和，这与softmax回归的主要区别在于，与softmax回归不同，softmax将单个标签分配给单个样本。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241002140128.png)

###适合多任务学习场景

第一，如果你训练的一组任务，可以共用低层次特征。对于无人驾驶的例子，同时识别交通灯、汽车和行人是有道理的，这些物体有相似的特征，也许能帮你识别停车标志，因为这些都是道路上的特征。

第二，这个准则没有那么绝对，所以不一定是对的。但我从很多成功的多任务学习案例中看到，如果每个任务的数据量很接近，你还记得迁移学习时，你从 A AA 任务学到知识然后迁移到 B BB 任务，所以如果任务 A AA 有1百万个样本，任务 B BB 只有1000个样本，那么你从这1百万个样本学到的知识，真的可以帮你增强对更小数据集任务 B BB 的训练。那么多任务学习又怎么样呢？在多任务学习中，你通常有更多任务而不仅仅是两个，所以也许你有，以前我们有4个任务，但比如说你要完成100个任务，而你要做多任务学习，尝试同时识别100种不同类型的物体。你可能会发现，每个任务大概有1000个样本。所以如果你专注加强单个任务的性能，比如我们专注加强第100个任务的表现，我们用 A 100 A100A100 表示，如果你试图单独去做这个最后的任务，你只有1000个样本去训练这个任务，这是100项任务之一，而通过在其他99项任务的训练，这些加起来可以一共有99000个样本，这可能大幅提升算法性能，可以提供很多知识来增强这个任务的性能。不然对于任务 A 100 A100A100 ，只有1000个样本的训练集，效果可能会很差。如果有对称性，这其他99个任务，也许能提供一些数据或提供一些知识来帮到这100个任务中的每一个任务。所以第二点不是绝对正确的准则，但我通常会看的是如果你专注于单项任务，如果想要从多任务学习得到很大性能提升，那么其他任务加起来必须要有比单个任务大得多的数据量。要满足这个条件，其中一种方法是，比如右边这个例子这样，或者如果每个任务中的数据量很相近，但关键在于，如果对于单个任务你已经有1000个样本了，那么对于所有其他任务，你最好有超过1000个样本，这样其他任务的知识才能帮你改善这个任务的性能。

最后多任务学习往往在以下场合更有意义，当你可以训练一个足够大的神经网络，同时做好所有的工作，所以多任务学习的替代方法是为每个任务训练一个单独的神经网络。所以不是训练单个神经网络同时处理行人、汽车、停车标志和交通灯检测。你可以训练一个用于行人检测的神经网络，一个用于汽车检测的神经网络，一个用于停车标志检测的神经网络和一个用于交通信号灯检测的神经网络。那么研究员Rich Carona几年前发现的是什么呢？多任务学习会降低性能的唯一情况，和训练单个神经网络相比性能更低的情况就是你的神经网络还不够大。但如果你可以训练一个足够大的神经网络，那么多任务学习肯定不会或者很少会降低性能，我们都希望它可以提升性能，比单独训练神经网络来单独完成各个任务性能要更好。
​