#5.6dropout 正则化

dropout就是对神经网络进行一个简单的__随机消除影响__

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1726666725450.png)

###inverted dropout
inverted dropout

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1726667104759.png)

设置一个keep-prob变量，作为这个神经元保留下来的可能性，设置一个di丢弃矩阵，i是对应的神经网络层数，di的维度是(ai的列和行)

这个生成<keep-prob的意思就是随机生成0-1的数，小于0.8就位true，大于0.8就位false，也就实现了__筛选__

最后a3/=keep-prob控制了最后输出的__期望__