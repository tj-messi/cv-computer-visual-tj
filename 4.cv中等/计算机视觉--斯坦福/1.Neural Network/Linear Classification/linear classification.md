#linear classification

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1728462880568.png)

##线性分类

在上一节中，我们介绍了图像分类问题，即为一组固定类别中的图像分配单个标签的任务。此外，我们描述了 k 最近邻 （kNN） 分类器，它通过将图像与训练集中的（注释的）图像进行比较来标记图像。正如我们所看到的，kNN 有许多缺点：

分类器必须记住所有训练数据并将其存储起来，以便将来与测试数据进行比较。这**效率低下**，因为数据集的大小很容易达到 GB。
对测试图像进行分类的**成本很高**，因为它需要与所有训练图像进行比较。

概述。我们现在将开发一种更强大的图像分类方法，最终会自然而然地扩展到整个神经网络和卷积神经网络。该方法将有两个主要组成部分：一个将原始数据映射到类分数的**评分函数**，以及一个量化预测分数和真实标签之间一致性的**损失函数**。然后，我们将它转换为一个优化问题，其中我们将最小化相对于 score 函数参数的损失函数

##从图像到标签分数的参数化映射

此方法的第一个组件是定义 score 函数，该函数将图像的像素值映射到每个类的置信度分数。我们将通过一个具体的例子来开发该方法。和以前一样，我们假设一个图像的训练数据集x我∈RD
，每个都与一个标签相关联y我
.这里i=1...N
和y我∈1...K
.也就是说，我们有 N 个示例（每个示例的维度为 D）和 K 个不同的类别。例如，在 CIFAR-10 中，我们有一个 N = 50,000 张图像的训练集，每张图像的 D = 32 x 32 x 3 = 3072 像素，K = 10，因为有 10 个不同的类别（狗、猫、汽车等）。我们现在将定义 score 函数f:RD↦RK
将原始图像像素映射到类分数。

线性分类器。在本模块中，我们将从可以说是最简单的函数开始，线性映射：

f(x我,W，b)=Wx我+b
在上面的方程式中，我们假设图像x我
将其所有像素展平为形状为 [D x 1] 的单列向量。矩阵 W（大小为 [K x D]）和向量 b（大小为 [K x 1]）是函数的参数。在 CIFAR-10 中，x我
包含第 i 个图像中拼合为单个 [3072 x 1] 列的所有像素，W 为 [10 x 3072]，B 为 [10 x 1]，因此函数中有 3072 个数字（原始像素值），有 10 个数字（类分数）。W 中的参数通常称为权重，而 b 中的参数称为偏差向量，因为它会影响输出分数，但不与实际数据交互x我
.但是，您经常会听到人们交替使用术语 weights 和 parameters。

**预示：卷积神经网络将图像像素映射到分数，如上所示，但映射 （ f ） 将更加复杂，并且将包含更多参数。**

##解释线性分类器
请注意，线性分类器将类的分数计算为类的所有 3 个颜色通道中所有像素值的加权和。根据我们为这些权重设置的值，该函数能够在图像中的某些位置喜欢或不喜欢（取决于每个权重的符号）某些颜色。例如，您可以想象，如果图像的侧面有很多蓝色（可能对应于水），则 “ship” 类的可能性更大。您可能期望 “ship” 分类器在其蓝色通道权重中具有大量正权重（蓝色的存在会增加船舶的分数），而在红/绿通道中具有负权重（红色/绿色的存在会降低船舶的分数）。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241009163947.png)

将图像类比为高维点。由于图像被拉伸成高维列向量，我们可以将每张图像解释为该空间中的单个点（例如，CIFAR-10 中的每张图像都是 32x32x3 像素的 3072 维空间中的一个点）。类似地，整个数据集是一组（标记的）点。

由于我们将每个类的分数定义为所有图像像素的加权和，因此每个类分数都是此空间上的线性函数。我们无法可视化 3072 维空间，但如果我们想象将所有这些维度压缩为仅两个维度，那么我们可以尝试可视化分类器可能正在做什么：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241009164024.png)

正如我们在上面看到的，每一行W
是其中一个类的分类器。这些数字的几何解释是，当我们更改W
，像素空间中的相应线条将向不同方向旋转。偏见b
，则允许我们的分类器翻译这些行。特别要注意的是，如果没有偏见项，请插入x我=0
无论权重如何，都将始终给出零分，因此所有线都将被迫穿过原点。

将线性分类器解释为模板匹配。权重的另一种解释W
是每行W
对应于其中一个类的模板（有时也称为原型）。然后，通过使用内积（或点积）将每个模板与图像逐一进行比较，以找到最适合的模板，从而获得图像的每个类别的分数。使用此术语，线性分类器正在进行模板匹配，其中学习模板。另一种思考方式是，我们仍然有效地进行最近邻，但我们没有拥有数千张训练图像，而是每个类只使用一张图像（尽管我们会学习它，而且它不一定是训练集中的图像之一），并且我们**使用（负）内积**作为距离，而不是 L1 或 L2 距离

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241009164100.png)

此外，请注意，马模板似乎包含一匹双头马，这是因为数据集中同时面向左侧和右侧的马。线性分类器将数据中的这两种马匹模式合并到一个模板中。同样，汽车分类器似乎已经将多种模式合并到一个模板中，该模板必须识别来自各个侧面和所有颜色的汽车。特别是，这个模板最终是红色的，这表明 CIFAR-10 数据集中的红色汽车比任何其他颜色的红色汽车都多。线性分类器太弱，无法正确解释不同颜色的汽车，但正如我们稍后将看到的，神经网络将允许我们执行这项任务。展望未来，神经网络将能够在其隐藏层中开发中间神经元，这些神经元可以检测特定的汽车类型（例如，绿色汽车面向左侧、蓝色汽车面向前方等），下一层的神经元可以通过单个汽车检测器的加权和将这些组合成更准确的汽车分数。

偏倚技巧。在继续之前，我们想提一个常见的简化技巧来表示这两个参数W，b
作为一个整体。回想一下，我们将 score 函数定义为：

f(x我,W，b)=Wx我+b
当我们继续浏览材料时，跟踪两组参数（偏差b
和权重W
） 中。一个常用的技巧是通过扩展向量将两组参数组合成一个矩阵，该矩阵包含它们x我
具有一个始终保持恒定的附加维度1
- 默认偏差维度。对于额外的维度，新的 score 函数将简化为单个矩阵乘法：

f(x我,W)=Wx我
在我们的 CIFAR-10 示例中，x我
现在是 [3073 x 1] 而不是 [3072 x 1] -（额外的维度保持常数 1），并且W
现在是 [10 x 3073] 而不是 [10 x 3072]。额外的列W
现在对应于 biasb
.插图可能有助于阐明：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241009164128.png)

**图像数据预处理**。快速说明一下，在上面的示例中，我们使用了原始像素值（范围从 [0...255]）。在机器学习中，始终对输入特征执行规范化是一种非常常见的做法（在图像的情况下，每个像素都被视为一个特征）。特别是，通过从每个特征中减去平均值来使数据居中非常重要。对于图像，这对应于计算训练图像的平均图像，并从每个图像中减去它，以获得像素范围约为 [-127 ...进一步的常见预处理是缩放每个输入特征，使其值范围从 [-1， 1]。其中，零均值居中可以说更重要，但我们必须等待它的证明，直到我们了解梯度下降的动力学

##损失函数

在上一节中，我们定义了一个从像素值到类分数的函数，该函数由一组权重参数化W
.此外，我们发现我们无法控制数据(x我,y我)
（它是固定的和给定的），但我们确实可以控制这些权重，并且我们希望设置它们，以便预测的类分数与训练数据中的真值标签一致。

例如，回到猫的示例图像及其在 “cat”、“dog” 和 “ship” 类中的分数，我们看到该示例中的特定权重集根本不是很好：我们输入了描绘猫的像素，但与其他类别（狗分数 437.9 和船分数 61.95）相比，猫分数非常低 （-96.8）。我们将用损失函数（或有时也称为成本函数或目标）来衡量我们对结果的不满，例如这个结果。直观地说，如果我们在对训练数据进行分类方面做得不好，损失会很高，如果我们做得好，损失会很低。

##Multiclass Support Vector Machine 损失（向量机）

有几种方法可以定义损失函数的详细信息。作为第一个例子，我们将首先开发一种常用的损失，称为多类支持向量机 （SVM） 损失。SVM 损失的设置是为了让 SVM“希望”每个图像的正确类的分数比不正确的类高出一些固定的边距Δ
.请注意，有时像我们上面所做的那样将损失函数拟人化是有帮助的：SVM“想要”某个结果，即结果会产生较低的损失（这很好）。

现在让我们更精确。回想一下，对于第 i 个示例，我们给定了图像的像素x我
和标签y我
，该 ID 指定正确类的索引。score 函数获取像素并计算向量f(x我,W)
的班级分数，我们将缩写为s
（分数的缩写）。例如，第 j 个类的分数是第 j 个元素：sj=f(x我,W)j
.然后，第 i 个示例的多类 SVM 损失正式化如下：

L我=∑J≠y我max（0，sj−sy我+Δ)
例。让我们通过一个示例来解读它，看看它是如何工作的。假设我们有三个类接收分数s=[13，−7,11]
，并且第一个类是真正的类（即y我=0
).还要假设Δ
（我们稍后将更详细地介绍一个超参数）是 10。上面的表达式对所有不正确的类 （J≠y我
），所以我们得到两个项：

L我=max（0，−7−13+10）+max（0,11−13+10）)
您可以看到，第一项给出零，因为 [-7 - 13 + 10] 给出一个负数，然后使用max（0，−）
功能。我们对这对的损失为零，因为正确的类分数 （13） 比错误的类分数 （-7） 至少大 10 倍。事实上，差值为 20，远大于 10，但 SVM 只关心差值至少为 10;超出 margin 的任何其他差异都将在 max 操作中钳制为零。第二项计算 [11 - 13 + 10]，得到 8。也就是说，即使正确类的分数高于错误类 （13 > 11），它也不会高出所需的边距 10。差额只有 2，这就是为什么亏损为 8（即差额必须高出多少才能满足抽水）。总之，SVM 损失函数需要正确类的分数y我
比错误的班级分数至少大Δ
（增量）。如果不是这种情况，我们将累积损失。

请注意，在这个特定的模块中，我们使用的是线性得分函数 （f(x我;W)=Wx我
），因此我们也可以将 loss 函数改写为这种等效形式：

L我=∑J≠y我max（0，wTjx我−wTy我x我+Δ)
哪里wj
是 的第 j 行W
重塑为柱。然而，一旦我们开始考虑更复杂的 score 函数形式，情况就不一定如此f
.

在结束本节之前，我们要提到的最后一点术语是 0 的阈值max（0，−）
函数通常称为铰链损失。您有时会听到人们使用平方铰链损失 SVM（或 L2-SVM）来代替，它使用max（0，−)2
这会更强烈地惩罚被侵犯的边距（二次方而不是线性）。未平方版本更标准，但在某些数据集中，平方铰链损失可以更好地工作。这可以在交叉验证期间确定。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241009164547.png)

多类支持向量机“希望”正确类的分数至少比所有其他分数高 delta 的幅度。如果任何类别的分数在红色区域（或更高）内，则会出现累积损失。否则，损失将为零。我们的目标是找到训练数据中所有样本同时满足此约束的权重，并给出尽可能低的总损失。

**正则化**。我们上面介绍的损失函数有一个 bug。假设我们有一个数据集和一组参数 W，它们可以正确地对每个样本进行分类（即所有分数都是为了满足所有边际，并且L我=0
对于所有 i）。问题在于这组 W 不一定是唯一的：可能有许多类似的 W 可以正确地对示例进行分类。一种简单的方法是，如果某些参数 W 正确地对所有样本进行分类（因此每个样本的损失为零），那么这些参数的任意倍数λW
哪里λ>1
也会得到零损失，因为这种变换均匀地拉伸了所有分数幅度，因此也拉伸了它们的绝对差值。例如，如果正确类和最接近的错误类之间的分数差为 15，则将 W 的所有元素乘以 2 将使新的差值为 30。

换句话说，我们希望对一组特定的权重 W 进行编码，以消除这种歧义。我们可以通过使用正则化惩罚来扩展损失函数来做到这一点 R(W)
.最常见的正则化惩罚是平方 L2 范数，它通过对所有参数的元素二次惩罚来阻止大权重：

R(W)=∑k∑lW2k，l
在上面的表达式中，我们总结了W
.请注意，正则化函数不是数据的函数，它仅基于权重。包括正则化损失将完成完整的多类支持向量机损失，它由两个部分组成：数据丢失（即平均损失L我
在所有示例中）和正则化损失。也就是说，完整的 Multiclass SVM 损失变为：

L=1N∑我L我数据丢失+λR(W)正则化损失
或者以完整形式扩展它：

L=1N∑我∑J≠y我[max（0，f(x我;W)j−f(x我;W)y我+Δ)]+λ∑k∑lW2k，l

哪里N
是训练样本的数量。如您所见，我们将正则化惩罚附加到 loss 目标，由超参数加权λ
.没有简单的方法来设置这个超参数，它通常由交叉验证确定。

除了我们上面提供的动机之外，还有许多理想的属性可以包括正则化惩罚，其中许多我们将在后面的部分中返回。例如，事实证明，包括 L2 惩罚会导致 SVM 中具有吸引人的最大保证金属性（如果您有兴趣，请参阅 CS229 讲义了解完整详细信息）。

最吸引人的特性是，惩罚大权重往往会提高泛化，因为这意味着没有输入维度本身可以对分数产生非常大的影响。例如，假设我们有一些输入向量x=[1,1,1,1]
和两个权重向量w1=[ 1,0,0,0 ]
,w2=[0.25,0.25,0.25,0.25 ]
.然后wT1x=wT2x=1
因此，两个权重向量导致相同的点积，但 L2 惩罚w1
为 1.0，而 L2 惩罚为w2
只有 0.5。因此，根据 L2 penalty 权重向量w2
将是首选，因为它实现了较低的正则化损失。直观地说，这是因为w2
更小且更分散。由于 L2 惩罚更喜欢更小、更分散的权重向量，因此鼓励最终分类器将所有输入维度考虑为少量，而不是几个输入维度，并且非常强烈。正如我们稍后将在本课程中看到的那样，这种效果可以提高分类器在测试图像上的泛化性能，并减少过拟合。

请注意，偏差不具有相同的效果，因为与权重不同，它们不控制输入维度的影响强度。因此，通常只正则化权重W
但不是偏见b
.然而，在实践中，这往往被证明影响可以忽略不计。最后，请注意，由于正则化惩罚，我们永远无法在所有示例中实现正好 0.0 的损失，因为这只有在W=0
.

代码。以下是在 Python 中实现的损失函数（无正则化），包括未矢量化和半矢量化形式


设置 Delta。请注意，我们刷过了超参数Δ
及其设置。它应该设置为什么值，我们是否必须对其进行交叉验证？事实证明，这个超参数可以安全地设置为Δ=1.0
在所有情况下。超参数Δ
和λ
似乎是两个不同的超参数，但实际上它们都控制着相同的权衡：目标中数据丢失和正则化损失之间的权衡。理解这一点的关键是权重W
对分数有直接影响（因此也对它们的差异有直接影响）：当我们缩小W
分数差异会变得更小，随着我们扩大权重，分数差异都会变得更大。因此，分数之间边距的确切值（例如Δ=1
或δ=100
） 在某种意义上毫无意义，因为权重可以任意缩小或拉伸差异。因此，唯一真正的权衡是我们允许权重增长多大（通过正则化强度λ
).

与二进制支持向量机的关系。您可能具有二进制支持向量机方面的经验，其中第 i 个示例的损失可以写为：

L我=Cmax（0,1-y我wTx我)+R(W)
哪里C
是超参数，而y我∈{−1,1}
.您可以说服自己，我们在本节中介绍的公式包含二进制 SVM 作为特殊情况，此时只有两个类。也就是说，如果我们只有两个类，那么损失就会减少到上面显示的二进制 SVM。也C
在此公式中，并且λ
在我们的公式控制中，相同的权衡 和 通过互惠关系相关C∝1λ
.

旁白：原始中的优化。如果您之前了解 SVM，那么您可能还听说过内核、对偶、SMO 算法等。在这门课中（就像一般的神经网络一样），我们将始终以不受约束的原始形式处理优化目标。其中许多目标在技术上是不可微分的（例如，max（x，y） 函数不是因为它在 x=y 时有扭结），但在实践中这不是问题，使用次梯度是很常见的。

旁白：其他 Multiclass SVM 公式。值得注意的是，本节中介绍的 Multiclass SVM 是在多个类上构建 SVM 的少数几种方法之一。另一种常用的形式是一对多 （OVA） SVM，它为每个类训练一个独立的二进制 SVM，而不是所有其他类。与此相关但在实践中不太常见的是 All-vs-All （AVA） 策略。我们的公式遵循 Weston 和 Watkins 1999 （pdf） 版本，该版本比 OVA 更强大（从某种意义上说，您可以构建多类数据集，其中此版本可以实现零数据丢失，但 OVA 不能。如果有兴趣，请参阅论文中的详细信息）。您可能看到的最后一个公式是结构化 SVM，它使正确类的分数与得分最高的错误亚军类的分数之间的边距最大化。了解这些表述之间的差异超出了本课程的范围。这些说明中介绍的版本在实践中使用是一个安全的赌注，但可以说最简单的 OVA 策略可能同样有效（正如 Rikin 等人在 2004 年在 In Defense of One-V-All Classification （pdf） 中也论证的那样）。

##softmax分类器

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1728463733494.png)

事实证明，SVM 是两个常见的分类器之一。另一个流行的选择是 Softmax 分类器，它具有不同的损失函数。如果您以前听说过二进制 Logistic Regression 分类器，那么 Softmax 分类器是它对多个类的泛化。与 SVM 不同，SVM 将输出f(x我,W)
作为每个类的（未校准且可能难以解释）分数，Softmax 分类器给出了稍微更直观的输出（标准化类概率），并且还具有我们稍后将描述的概率解释。在 Softmax 分类器中，函数 mappingf(x我;W)=Wx我
保持不变，但我们现在将这些分数解释为每个类的非归一化对数概率，并将铰链损失替换为以下形式的交叉熵损失：

##softmax和svm区别

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241009165019.png)

Softmax 分类器为每个类提供 “probabilities”。与计算所有类的未校准且不易解释分数的 SVM 不同，Softmax 分类器允许我们计算所有标签的“概率”。例如，给定一张图像，SVM 分类器可能会为类 “cat”、“dog” 和 “ship” 提供分数 [12.5， 0.6， -23.0]。相反，softmax 分类器可以将三个标签的概率计算为 [0.9， 0.09， 0.01]，这允许您解释它在每个类中的置信度。然而，我们将 “probabilities” 一词放在引号中的原因是，这些概率的峰值或扩散程度直接取决于正则化强度λ
- 您负责作为系统输入的内容。例如，假设某些三个类的非规范化对数概率为 [1， -2， 0]。然后，softmax 函数将计算：

[1，−2,0]→[e1,e−2,e0]=[2.71,0.14,1]→[0.7,0.04,0.26 ]
其中所采取的步骤是指数化和标准化以求和为 1。现在，如果正则化强度λ
更高，权重W
会受到更多的惩罚，这会导致权重更小。例如，假设权重变小了一半 （[0.5， -1， 0]）。softmax 现在将计算：

[0.5，−1,0]→[e0.5,e−1,e0]=[1.65,0.37,1]→[0.55,0.12,0.33 ]
其中的概率现在更加分散。此外，由于非常强的正则化强度，权重趋向于小数λ
，则输出概率将接近均匀。因此，最好将 Softmax 分类器计算的概率视为置信度，其中与 SVM 类似，分数的排序是可解释的，但绝对数字（或其差异）在技术上不是。

