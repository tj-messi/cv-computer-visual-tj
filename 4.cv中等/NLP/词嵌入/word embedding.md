#word embedding

##词嵌入

机器理解文本的方式就是转化为向量

在NLP(自然语言处理)领域，文本表示是第一步，也是很重要的一步，通俗来说就是把人类的语言符号转化为机器能够进行计算的数字

文本表示分为**离散表示**和**分布式表示**：

##离散表示

###One-hot 向量

One-hot简称读热向量编码，也是特征工程中最常用的方法。其步骤如下：

构造文本分词后的字典，每个分词是一个比特值，比特值为0或者1。
每个分词的文本表示为该分词的比特位为1，其余位为0的矩阵表示。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1730380203070.png)

问题就是语料库增加这样会产生一个很稀疏维度很高的矩阵

这种表示方法的分词顺序和在句子中的顺序是无关的，不能保留词与词之间的关系信息

###词袋模型

词袋模型(Bag-of-words model)，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。

文档的向量表示可以直接将各词的词向量表示加和。例如：

John likes to watch movies. Mary likes too

John also likes to watch football games.

以上两句可以构造一个词典，**{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, "also": 6, "football": 7, "games": 8, "Mary": 9, "too": 10} **

那么第一句的向量表示为：[1,2,1,1,1,0,0,0,1,1]，其中的2表示likes在该句中出现了2次，依次类推。

词袋模型同样有一下缺点：

词向量化后，词与词之间是有大小关系的，不一定词出现的越多，权重越大。
词与词之间是没有顺序关系的。

###TF-IDF

**TF-IDF**（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。TF意思是词频(Term Frequency)，IDF意思是逆文本频率指数(Inverse Document Frequency)。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/1730466416076.png)

分母之所以加1，是为了避免分母为0。

那么，，从这个公式可以看出，当w在文档中出现的次数增大时，而TF-IDF的值是减小的，所以也就体现了以上所说的了。

**缺点：**还是没有把词与词之间的关系顺序表达出来。

###n-gram模型

n-gram模型为了保持词的顺序，做了一个滑窗的操作，这里的n表示的就是滑窗的大小，例如2-gram模型，也就是把2个词当做一组来处理，然后向后移动一个词的长度，再次组成另一组词，把这些生成一个字典，按照词袋模型的方式进行编码得到结果。改模型考虑了词的顺序。

例如：

John likes to watch movies. Mary likes too

John also likes to watch football games.

以上两句可以构造一个词典，{"John likes”: 1, "likes to”: 2, "to watch”: 3, "watch movies”: 4, "Mary likes”: 5, "likes too”: 6, "John also”: 7, "also likes”: 8, “watch football”: 9, "football games": 10}

那么第一句的向量表示为：[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]，其中第一个1表示John likes在该句中出现了1次，依次类推。

**缺点：**随着n的大小增加，词表会成指数型膨胀，会越来越大。

###2.5离散表示的问题

由于存在以下的问题，对于一般的NLP问题，是可以使用离散表示文本信息来解决问题的，但对于要求精度较高的场景就不适合了。

无法衡量词向量之间的关系。

词表的维度随着语料库的增长而膨胀。

n-gram词序列随语料库增长呈指数型膨胀，更加快。

离散数据来表示文本会带来数据稀疏问题，导致丢失了信息，与我们生活中理解的信息是不一样的

##3 分布式表示

**用一个词附近的其它词来表示该词，这是现代统计自然语言处理中最有创见的想法之一**

###3.1 共现矩阵

共现矩阵顾名思义就是共同出现的意思，词文档的共现矩阵主要用于发现主题(topic)，用于主题模型，如LSA

局域窗中的word-word共现矩阵可以挖掘语法和语义信息，例如：

I like deep learning.

I like NLP.

I enjoy flying

有以上三句话，设置滑窗为2，可以得到一个词典：{"I like","like deep","deep learning","like NLP","I enjoy","enjoy flying","I like"}。

我们可以得到一个共现矩阵(对称矩阵)：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241107181737.png)

中间的每个格子表示的是行和列组成的词组在词典中共同出现的次数，也就体现了共现的特性

存在的问题：

1.向量维数随着词典大小线性增长。

2.存储整个词典的空间消耗非常大。

3.一些模型如文本分类模型会面临稀疏性问题。

4.模型会欠稳定，每新增一份语料进来，稳定性就会变化。

##4.神经网络

###4.1 NNLM

**NNLM (Neural Network Language model)**，神经网络语言模型是03年提出来的，通过训练得到中间产物--词向量矩阵，这就是我们要得到的文本表示向量矩阵

NNLM说的是定义一个前向窗口大小，其实和上面提到的窗口是一个意思。把这个窗口中最后一个词当做y，把之前的词当做输入x，通俗来说就是预测这个窗口中最后一个词出现概率的模型

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241107182036.png)

以下是NNLM的网络结构图:

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241107182126.png)

**input层**是一个前向词的输入，是经过one-hot编码的词向量表示形式，具有V*1的矩阵

C矩阵是投影矩阵，也就是稠密词向量表示，在神经网络中是w参数矩阵，该矩阵的大小为D*V，正好与input层进行全连接(相乘)得到D*1的矩阵，采用线性映射将one-hot表示投影到稠密D维表示

**output层(softmax)**自然是前向窗中需要预测的词

通过BP＋SGD得到最优的C投影矩阵，这就是NNLM的中间产物，也是我们所求的文本表示矩阵，通过NNLM将稀疏矩阵投影到稠密向量矩阵中。

###4.2 word2vec

谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，**分别是CBOW（Continues Bag of Words）连续词袋和Skip-gram。**Word2Vec和上面的NNLM很类似，但比NNLM简单

####CBOW
CBOW获得中间词两边的的上下文，然后用周围的词去预测中间的词，把中间词当做y，把窗口中的其它词当做x输入，x输入是经过one-hot编码过的，然后通过一个隐层进行求和操作，最后通过激活函数softmax，可以计算出每个单词的生成概率，接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化，而求得的权重矩阵就是文本表示词向量的结果

####Skip-gram

Skip-gram是通过当前词来预测窗口中上下文词出现的概率模型，把当前词当做x，把窗口中其它词当做y，依然是通过一个隐层接一个Softmax激活函数来预测其它词的概率。如下图所示：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241107182859.png)

优化方法：

层次Softmax：至此还没有结束，因为如果单单只是接一个softmax激活函数，计算量还是很大的，有多少词就会有多少维的权重矩阵，所以这里就提出层次Softmax(Hierarchical Softmax)，使用Huffman Tree来编码输出层的词典，相当于平铺到各个叶子节点上，瞬间把维度降低到了树的深度，可以看如下图所示。这课Tree把出现频率高的词放到靠近根节点的叶子节点处，每一次只要做二分类计算，计算路径上所有非叶子节点词向量的贡献即可。

###4.3 sense2vec
word2vec模型的问题在于词语的多义性。比如duck这个单词常见的含义有水禽或者下蹲，但对于 word2vec 模型来说，它倾向于将所有概念做归一化平滑处理，得到一个最终的表现形式