#attention
#一、注意力机制：Attention
##1.1 什么是注意力机制？
看人-->看脸
看文章-->看标题
看段落-->看开头
这时候大家应该大致知道注意力机制是个什么东西了吧~

注意力机制其实是源自于人对于外部信息的处理能力。由于人每一时刻接受的信息都是无比的庞大且复杂，远远超过人脑的处理能力，因此人在处理信息的时候，会将注意力放在需要关注的信息上，对于其他无关的外部信息进行过滤，这种处理方式被称为注意力机制。

我用通俗的大白话解释一下：注意力呢，对于我们人来说可以理解为“关注度”，对于没有感情的机器来说其实就是赋予多少权重(比如0-1之间的小数)，越重要的地方或者越相关的地方就赋予越高的权重。
##1.2 如何运用注意力机制？
###1.2.1 Query&Key&Value
**查询（Query）**： 指的是查询的范围，自主提示，即主观意识的特征向量

**键（Key）**： 指的是被比对的项，非自主提示，即物体的突出特征信息向量

**值（Value）** ：  则是代表物体本身的特征向量，通常和Key成对出现

注意力机制是通过Query与Key的注意力汇聚（给定一个 Query，计算Query与 Key的相关性，然后根据Query与Key的相关性去找到最合适的 Value）实现对Value的注意力权重分配，生成最终的输出结果

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121156.png)

###1.2.2 注意力机制计算过程

**阶段一：**根据Query和Key计算两者之间的相关性或相似性（常见方法点积、余弦相似度，MLP网络），得到注意力得分

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121358.png)

**阶段二**：对注意力得分进行缩放scale（除以维度的根号），再softmax函数，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过softmax的内在机制更加突出重要元素的权重。一般采用如下公式计算：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121444.png)

**阶段三**：根据权重系数对Value值进行加权求和，得到Attention Value（此时的V是具有一些注意力信息的，更重要的信息更关注，不重要的信息被忽视了）

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121512.png)

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121542.png)

#自注意力机制：Self-Attention
##2.1 什么是自注意力机制？

自注意力机制实际上是注意力机制中的一种，也是一种网络的构型，它想要解决的问题是神经网络接收的输入是很多大小不一的向量，并且不同向量向量之间有一定的关系，但是实际训练的时候无法充分发挥这些输入之间的关系而导致模型训练结果效果极差。比如机器翻译(序列到序列的问题，机器自己决定多少个标签)，词性标注(Pos tagging一个向量对应一个标签)，语义分析(多个向量对应一个标签)等文字处理问题。

针对全连接神经网络对于多个相关的输入无法建立起相关性的这个问题，通过自注意力机制来解决，自注意力机制实际上是想让机器注意到整个输入中不同部分之间的相关性。

自注意力机制是注意力机制的变体，其减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性。自注意力机制的关键点在于，Q、K、V是同一个东西，或者三者来源于同一个X，三者同源。通过X找到X里面的关键点，从而更关注X的关键信息，忽略X的不重要信息。**不是输入语句和输出语句之间的注意力机制，而是输入语句内部元素之间或者输出语句内部元素之间发生的注意力机制。**

	注意力机制和自注意力机制的区别：
	
	 （1）注意力机制的Q和K是不同来源的，例如，在Encoder-Decoder模型中，K是Encoder中的元素，而Q是Decoder中的元素。在中译英模型中，中文句子通过编码器被转化为一组特征表示K，这些特征表示包含了输入中文句子的语义信息。解码器在生成英文句子时，会使用这些特征表示K以及当前生成的英文单词特征Q来决定下一个英文单词是什么。
	
	（2）自注意力机制的Q和K则都是来自于同一组的元素，例如，在Encoder-Decoder模型中，Q和K都是Encoder中的元素，即Q和K都是中文特征，相互之间做注意力汇聚。也可以理解为同一句话中的词元或者同一张图像中不同的patch，这都是一组元素内部相互做注意力机制，因此，自注意力机制（self-attention）也被称为内部注意力机制（intra-attention）。

##2.2 如何运用自注意力机制？

**第1步**：得到Q，K，V的值

对于每一个向量x，分别乘上三个系数 W^{q}， W^{k}，W^{v}，得到的Q，K和V分别表示query，key和value

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016122054.png)

这个三个W就是可学习的系数

**第2步**：Matmul

利用得到的Q和K计算每两个输入向量之间的相关性，一般采用点积计算，为每个向量计算一个score：score =q · k 

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016122713.png)

**第3步**：Scale+Softmax

将刚得到的相似度除以\sqrt{d_{k}}，再进行Softmax。经过Softmax的归一化后，每个值是一个大于0且小于1的权重系数，且总和为1，这个结果可以被理解成一个权重矩阵。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016122733.png)

**第4步**：Matmul

使用刚得到的权重矩阵，与V相乘，计算加权求和。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016122830.png)

##2.3 自注意力问题

自注意力机制的原理是筛选重要信息，过滤不重要信息，这就导致其有效信息的抓取能力会比CNN小一些。这是因为自注意力机制相比CNN，无法利用图像本身具有的尺度，平移不变性，以及图像的特征局部性（图片上相邻的区域有相似的特征，即同一物体的信息往往都集中在局部）这些先验知识，只能通过大量数据进行学习。这就导致自注意力机制只有在大数据的基础上才能有效地建立准确的全局关系，而在小数据的情况下，其效果不如CNN。

另外，自注意力机制虽然考虑了所有的输入向量，但没有考虑到向量的位置信息。在实际的文字处理问题中，可能在不同位置词语具有不同的性质，比如动词往往较低频率出现在句首。

要唠这个这就唠到位置编码(Positional Encoding) 了，这个我们下篇论文里面再讲，先大致说一下吧：对每一个输入向量加上一个位置向量e，位置向量的生成方式有多种，通过e来表示位置信息带入self-attention层进行计算。

#三、多头注意力机制：Multi-Head Self-Attention

##3.1 什么是多头注意力机制？

在实践中，当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 子空间表示（representation subspaces）可能是有益的。

为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的h组（一般h=8）不同的线性投影（linear projections）来变换查询、键和值。 然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这h个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为多头注意力（multihead attention）

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016123044.png)

##3.2 如何运用多头注意力机制？

**第1步**：定义多组W，生成多组Q、K、V

刚才我们已经理解了，Q、K、V是输入向量X分别乘上三个系数 ， ，分别相乘得到的，  ， ，是可训练的参数矩阵。

现在，对于同样的输入X，我们定义多组不同的 ， ， ，比如、、，、、每组分别计算生成不同的Q、K、V，最后学习到不同的参数。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016123159.png)

**第2步**：定义8组参数

对应8个single head，对应8组  W^{q}， W^{k}，W^{v} ，再分别进行self-attention，就得到了Z_{0}-Z_{7}

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016123228.png)

**第3步**：将多组输出拼接后乘以矩阵W_{0}以降低维度

首先在输出到下一层前，我们需要将Z_{0}-Z_{7}concat到一起，乘以矩阵W_{0}做一次线性变换降维，得到Z。

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016123316.png)

完整流程：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016123324.png)