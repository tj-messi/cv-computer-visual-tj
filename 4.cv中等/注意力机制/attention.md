#attention
#一、注意力机制：Attention
##1.1 什么是注意力机制？
看人-->看脸
看文章-->看标题
看段落-->看开头
这时候大家应该大致知道注意力机制是个什么东西了吧~

注意力机制其实是源自于人对于外部信息的处理能力。由于人每一时刻接受的信息都是无比的庞大且复杂，远远超过人脑的处理能力，因此人在处理信息的时候，会将注意力放在需要关注的信息上，对于其他无关的外部信息进行过滤，这种处理方式被称为注意力机制。

我用通俗的大白话解释一下：注意力呢，对于我们人来说可以理解为“关注度”，对于没有感情的机器来说其实就是赋予多少权重(比如0-1之间的小数)，越重要的地方或者越相关的地方就赋予越高的权重。
##1.2 如何运用注意力机制？
###1.2.1 Query&Key&Value
**查询（Query）**： 指的是查询的范围，自主提示，即主观意识的特征向量

**键（Key）**： 指的是被比对的项，非自主提示，即物体的突出特征信息向量

**值（Value）** ：  则是代表物体本身的特征向量，通常和Key成对出现

注意力机制是通过Query与Key的注意力汇聚（给定一个 Query，计算Query与 Key的相关性，然后根据Query与Key的相关性去找到最合适的 Value）实现对Value的注意力权重分配，生成最终的输出结果

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121156.png)

###1.2.2 注意力机制计算过程

**阶段一：**根据Query和Key计算两者之间的相关性或相似性（常见方法点积、余弦相似度，MLP网络），得到注意力得分

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121358.png)

**阶段二**：对注意力得分进行缩放scale（除以维度的根号），再softmax函数，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过softmax的内在机制更加突出重要元素的权重。一般采用如下公式计算：

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121444.png)

**阶段三**：根据权重系数对Value值进行加权求和，得到Attention Value（此时的V是具有一些注意力信息的，更重要的信息更关注，不重要的信息被忽视了）

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121512.png)

![](https://cdn.jsdelivr.net/gh/tj-messi/picture/20241016121542.png)

#自注意力机制：Self-Attention
##2.1 什么是自注意力机制？

自注意力机制实际上是注意力机制中的一种，也是一种网络的构型，它想要解决的问题是神经网络接收的输入是很多大小不一的向量，并且不同向量向量之间有一定的关系，但是实际训练的时候无法充分发挥这些输入之间的关系而导致模型训练结果效果极差。比如机器翻译(序列到序列的问题，机器自己决定多少个标签)，词性标注(Pos tagging一个向量对应一个标签)，语义分析(多个向量对应一个标签)等文字处理问题。

针对全连接神经网络对于多个相关的输入无法建立起相关性的这个问题，通过自注意力机制来解决，自注意力机制实际上是想让机器注意到整个输入中不同部分之间的相关性。

自注意力机制是注意力机制的变体，其减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性。自注意力机制的关键点在于，Q、K、V是同一个东西，或者三者来源于同一个X，三者同源。通过X找到X里面的关键点，从而更关注X的关键信息，忽略X的不重要信息。**不是输入语句和输出语句之间的注意力机制，而是输入语句内部元素之间或者输出语句内部元素之间发生的注意力机制。**

	注意力机制和自注意力机制的区别：
	
	 （1）注意力机制的Q和K是不同来源的，例如，在Encoder-Decoder模型中，K是Encoder中的元素，而Q是Decoder中的元素。在中译英模型中，中文句子通过编码器被转化为一组特征表示K，这些特征表示包含了输入中文句子的语义信息。解码器在生成英文句子时，会使用这些特征表示K以及当前生成的英文单词特征Q来决定下一个英文单词是什么。
	
	（2）自注意力机制的Q和K则都是来自于同一组的元素，例如，在Encoder-Decoder模型中，Q和K都是Encoder中的元素，即Q和K都是中文特征，相互之间做注意力汇聚。也可以理解为同一句话中的词元或者同一张图像中不同的patch，这都是一组元素内部相互做注意力机制，因此，自注意力机制（self-attention）也被称为内部注意力机制（intra-attention）。

##2.2 如何运用自注意力机制？

**第1步**：得到Q，K，V的值

对于每一个向量x，分别乘上三个系数 W^{q}， W^{k}，W^{v}，得到的Q，K和V分别表示query，key和value